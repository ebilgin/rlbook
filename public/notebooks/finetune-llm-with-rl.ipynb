{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Your First LLM with Reinforcement Learning\n",
    "\n",
    "This notebook walks you through the complete RLHF pipeline:\n",
    "1. **Supervised Fine-Tuning (SFT)** - Teaching format\n",
    "2. **Reward Modeling** - Learning preferences\n",
    "3. **PPO Training** - Optimizing for quality\n",
    "\n",
    "**Requirements:**\n",
    "- Free Colab GPU (T4)\n",
    "- ~1-2 hours runtime\n",
    "\n",
    "**What you'll learn:**\n",
    "- How each RLHF stage works\n",
    "- The role of the KL penalty\n",
    "- How to detect reward hacking\n",
    "\n",
    "This notebook accompanies the guide at [rlbook.ai/applications/finetune-llm](https://rlbook.ai/applications/finetune-llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets accelerate peft trl bitsandbytes\n",
    "!pip install -q torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"  # Small model for learning, fits on free Colab\n",
    "\n",
    "# Training configuration\n",
    "@dataclass\n",
    "class Config:\n",
    "    # SFT\n",
    "    sft_epochs: int = 3\n",
    "    sft_lr: float = 1e-4\n",
    "    sft_batch_size: int = 2\n",
    "    \n",
    "    # Reward Model\n",
    "    rm_epochs: int = 5\n",
    "    rm_lr: float = 1e-4\n",
    "    rm_batch_size: int = 2\n",
    "    \n",
    "    # PPO\n",
    "    ppo_iterations: int = 20\n",
    "    ppo_epochs: int = 4\n",
    "    ppo_lr: float = 1e-5\n",
    "    ppo_batch_size: int = 4\n",
    "    clip_range: float = 0.2\n",
    "    kl_coef: float = 0.1\n",
    "    target_kl: float = 0.02\n",
    "    \n",
    "    # Generation\n",
    "    max_length: int = 256\n",
    "    max_new_tokens: int = 100\n",
    "    temperature: float = 0.7\n",
    "    \n",
    "    # LoRA\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.1\n",
    "\n",
    "config = Config()\n",
    "print(\"Configuration loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply LoRA\n",
    "\n",
    "LoRA (Low-Rank Adaptation) allows us to fine-tune efficiently by only training a small fraction of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Check trainable parameters\n",
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Supervised Fine-Tuning (SFT)\n",
    "\n",
    "First, we teach the model the format of good Q&A responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training examples for SFT\n",
    "SFT_EXAMPLES = [\n",
    "    {\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"answer\": \"The capital of France is Paris.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does photosynthesis work?\",\n",
    "        \"answer\": \"Photosynthesis is the process by which plants convert sunlight, water, and carbon dioxide into glucose and oxygen. It occurs in chloroplasts using chlorophyll pigments.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is machine learning?\",\n",
    "        \"answer\": \"Machine learning is a subset of artificial intelligence where systems learn patterns from data rather than being explicitly programmed. Models improve through experience.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Why is the sky blue?\",\n",
    "        \"answer\": \"The sky appears blue because of Rayleigh scattering. Sunlight entering the atmosphere scatters off air molecules, with shorter blue wavelengths scattering more than longer red wavelengths.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is reinforcement learning?\",\n",
    "        \"answer\": \"Reinforcement learning is a type of machine learning where an agent learns to make decisions by taking actions in an environment and receiving rewards or penalties. The goal is to maximize cumulative reward over time.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is a neural network?\",\n",
    "        \"answer\": \"A neural network is a computational model inspired by biological neurons. It consists of layers of interconnected nodes that transform inputs into outputs through learned weights.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is gradient descent?\",\n",
    "        \"answer\": \"Gradient descent is an optimization algorithm that iteratively adjusts parameters in the direction of steepest decrease of a loss function. It is fundamental to training neural networks.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the purpose of an activation function?\",\n",
    "        \"answer\": \"Activation functions introduce non-linearity into neural networks, allowing them to learn complex patterns. Without them, multiple layers would collapse to a single linear transformation.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "def format_for_training(example):\n",
    "    \"\"\"Format Q&A pair as training text.\"\"\"\n",
    "    return f\"Question: {example['question']}\\n\\nAnswer: {example['answer']}\"\n",
    "\n",
    "# Preview\n",
    "print(\"Example formatted data:\")\n",
    "print(\"-\" * 40)\n",
    "print(format_for_training(SFT_EXAMPLES[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    \"\"\"Dataset for SFT training.\"\"\"\n",
    "    \n",
    "    def __init__(self, examples, tokenizer, max_length=256):\n",
    "        self.examples = examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = format_for_training(self.examples[idx])\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encodings[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encodings[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": encodings[\"input_ids\"].squeeze()\n",
    "        }\n",
    "\n",
    "# Create dataset and dataloader\n",
    "sft_dataset = QADataset(SFT_EXAMPLES, tokenizer, config.max_length)\n",
    "sft_dataloader = DataLoader(sft_dataset, batch_size=config.sft_batch_size, shuffle=True)\n",
    "\n",
    "print(f\"SFT dataset: {len(sft_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT Training\n",
    "optimizer = AdamW(model.parameters(), lr=config.sft_lr)\n",
    "model.train()\n",
    "\n",
    "print(\"Starting SFT training...\")\n",
    "for epoch in range(config.sft_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(sft_dataloader, desc=f\"Epoch {epoch+1}/{config.sft_epochs}\"):\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(sft_dataloader)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "print(\"SFT training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, question, max_new_tokens=100, temperature=0.7):\n",
    "    \"\"\"Generate a response to a question.\"\"\"\n",
    "    prompt = f\"Question: {question}\\n\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"Answer:\" in response:\n",
    "        response = response.split(\"Answer:\")[-1].strip()\n",
    "    return response\n",
    "\n",
    "# Test the SFT model\n",
    "print(\"Testing SFT model:\")\n",
    "print(\"-\" * 50)\n",
    "test_questions = [\n",
    "    \"What is Python?\",\n",
    "    \"How do computers work?\",\n",
    "    \"What is the meaning of life?\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {generate_response(model, tokenizer, q)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Reward Modeling\n",
    "\n",
    "Now we train a model to predict which responses are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_response(prompt, response):\n",
    "    \"\"\"\n",
    "    Heuristic scoring function to simulate human preferences.\n",
    "    In real RLHF, this comes from actual human annotations.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # Prefer complete sentences\n",
    "    if response.endswith(('.', '!', '?')):\n",
    "        score += 2\n",
    "    \n",
    "    # Prefer reasonable length\n",
    "    length = len(response)\n",
    "    if 20 <= length <= 200:\n",
    "        score += 2\n",
    "    elif length < 20:\n",
    "        score -= 2\n",
    "    elif length > 300:\n",
    "        score -= 1\n",
    "    \n",
    "    # Penalize filler phrases\n",
    "    filler_phrases = [\"I think\", \"maybe\", \"perhaps\", \"it depends\"]\n",
    "    for phrase in filler_phrases:\n",
    "        if phrase.lower() in response.lower():\n",
    "            score -= 1\n",
    "    \n",
    "    # Reward direct answers\n",
    "    if response and response[0].isupper():\n",
    "        score += 1\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate preference data\n",
    "TRAINING_PROMPTS = [\n",
    "    \"What is Python?\",\n",
    "    \"How do neural networks learn?\",\n",
    "    \"Explain gradient descent.\",\n",
    "    \"What is the purpose of activation functions?\",\n",
    "    \"How does backpropagation work?\",\n",
    "    \"What is overfitting?\",\n",
    "    \"Explain the bias-variance tradeoff.\",\n",
    "    \"What is a loss function?\",\n",
    "    \"How do transformers work?\",\n",
    "    \"What is attention in deep learning?\",\n",
    "]\n",
    "\n",
    "def create_preference_data(model, tokenizer, prompts, n_samples=4):\n",
    "    \"\"\"Generate responses and create preference pairs.\"\"\"\n",
    "    preference_data = []\n",
    "    \n",
    "    for prompt in tqdm(prompts, desc=\"Generating preferences\"):\n",
    "        # Generate multiple responses\n",
    "        responses = []\n",
    "        for _ in range(n_samples):\n",
    "            response = generate_response(model, tokenizer, prompt)\n",
    "            responses.append(response)\n",
    "        \n",
    "        # Create pairs with preferences\n",
    "        for i in range(len(responses)):\n",
    "            for j in range(i + 1, len(responses)):\n",
    "                score_i = score_response(prompt, responses[i])\n",
    "                score_j = score_response(prompt, responses[j])\n",
    "                \n",
    "                if score_i > score_j:\n",
    "                    chosen, rejected = responses[i], responses[j]\n",
    "                elif score_j > score_i:\n",
    "                    chosen, rejected = responses[j], responses[i]\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                preference_data.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"chosen\": chosen,\n",
    "                    \"rejected\": rejected\n",
    "                })\n",
    "    \n",
    "    return preference_data\n",
    "\n",
    "print(\"Generating preference data...\")\n",
    "preference_data = create_preference_data(model, tokenizer, TRAINING_PROMPTS, n_samples=4)\n",
    "print(f\"Created {len(preference_data)} preference pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview preference data\n",
    "if preference_data:\n",
    "    example = preference_data[0]\n",
    "    print(\"Example preference pair:\")\n",
    "    print(f\"Prompt: {example['prompt']}\")\n",
    "    print(f\"Chosen: {example['chosen'][:150]}...\")\n",
    "    print(f\"Rejected: {example['rejected'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    \"\"\"Reward model for RLHF.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model_name, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Load a fresh copy of the base model\n",
    "        self.base = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # Freeze base model\n",
    "        for param in self.base.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Add reward head\n",
    "        hidden_size = self.base.config.hidden_size\n",
    "        self.reward_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        ).to(device).half()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.base(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        hidden = outputs.hidden_states[-1]\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            seq_lengths = attention_mask.sum(dim=1) - 1\n",
    "            batch_size = hidden.shape[0]\n",
    "            last_hidden = hidden[torch.arange(batch_size, device=hidden.device), seq_lengths]\n",
    "        else:\n",
    "            last_hidden = hidden[:, -1, :]\n",
    "        \n",
    "        reward = self.reward_head(last_hidden).squeeze(-1)\n",
    "        return reward\n",
    "\n",
    "# Initialize reward model\n",
    "print(\"Initializing reward model...\")\n",
    "reward_model = RewardModel(MODEL_NAME)\n",
    "print(\"Reward model initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferenceDataset(Dataset):\n",
    "    \"\"\"Dataset for reward model training.\"\"\"\n",
    "    \n",
    "    def __init__(self, preference_data, tokenizer, max_length=256):\n",
    "        self.data = preference_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        chosen_text = f\"Question: {item['prompt']}\\n\\nAnswer: {item['chosen']}\"\n",
    "        chosen_enc = self.tokenizer(\n",
    "            chosen_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        rejected_text = f\"Question: {item['prompt']}\\n\\nAnswer: {item['rejected']}\"\n",
    "        rejected_enc = self.tokenizer(\n",
    "            rejected_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"chosen_ids\": chosen_enc[\"input_ids\"].squeeze(),\n",
    "            \"chosen_mask\": chosen_enc[\"attention_mask\"].squeeze(),\n",
    "            \"rejected_ids\": rejected_enc[\"input_ids\"].squeeze(),\n",
    "            \"rejected_mask\": rejected_enc[\"attention_mask\"].squeeze(),\n",
    "        }\n",
    "\n",
    "rm_dataset = PreferenceDataset(preference_data, tokenizer)\n",
    "rm_dataloader = DataLoader(rm_dataset, batch_size=config.rm_batch_size, shuffle=True)\n",
    "print(f\"Reward model dataset: {len(rm_dataset)} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train reward model\n",
    "rm_optimizer = AdamW(reward_model.reward_head.parameters(), lr=config.rm_lr)\n",
    "reward_model.train()\n",
    "\n",
    "print(\"Training reward model...\")\n",
    "for epoch in range(config.rm_epochs):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_pairs = 0\n",
    "    \n",
    "    for batch in tqdm(rm_dataloader, desc=f\"RM Epoch {epoch+1}/{config.rm_epochs}\"):\n",
    "        chosen_ids = batch[\"chosen_ids\"].to(reward_model.device)\n",
    "        chosen_mask = batch[\"chosen_mask\"].to(reward_model.device)\n",
    "        rejected_ids = batch[\"rejected_ids\"].to(reward_model.device)\n",
    "        rejected_mask = batch[\"rejected_mask\"].to(reward_model.device)\n",
    "        \n",
    "        r_chosen = reward_model(chosen_ids, chosen_mask)\n",
    "        r_rejected = reward_model(rejected_ids, rejected_mask)\n",
    "        \n",
    "        # Bradley-Terry loss\n",
    "        loss = -F.logsigmoid(r_chosen - r_rejected).mean()\n",
    "        \n",
    "        rm_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        rm_optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_correct += (r_chosen > r_rejected).sum().item()\n",
    "        total_pairs += len(r_chosen)\n",
    "    \n",
    "    accuracy = total_correct / total_pairs if total_pairs > 0 else 0\n",
    "    avg_loss = total_loss / len(rm_dataloader)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.2%}\")\n",
    "\n",
    "print(\"Reward model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: PPO Training\n",
    "\n",
    "Now we use RL to optimize the policy based on the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reference policy (frozen copy)\n",
    "print(\"Creating reference policy...\")\n",
    "ref_policy = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "for param in ref_policy.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"Reference policy created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePPOTrainer:\n",
    "    \"\"\"\n",
    "    Simplified PPO trainer for educational purposes.\n",
    "    Focuses on clarity over efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, policy, ref_policy, reward_model, tokenizer, config):\n",
    "        self.policy = policy\n",
    "        self.ref_policy = ref_policy\n",
    "        self.reward_model = reward_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        \n",
    "        self.optimizer = AdamW(\n",
    "            [p for p in self.policy.parameters() if p.requires_grad],\n",
    "            lr=config.ppo_lr\n",
    "        )\n",
    "        self.kl_coef = config.kl_coef\n",
    "    \n",
    "    def train_step(self, prompts: List[str]) -> Dict:\n",
    "        \"\"\"Complete training step: generate, compute rewards, update.\"\"\"\n",
    "        \n",
    "        self.policy.eval()\n",
    "        batch_data = []\n",
    "        \n",
    "        # Generate responses and collect data\n",
    "        for prompt in prompts:\n",
    "            formatted = f\"Question: {prompt}\\n\\nAnswer:\"\n",
    "            inputs = self.tokenizer(formatted, return_tensors=\"pt\").to(self.policy.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Generate with policy\n",
    "                outputs = self.policy.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=self.config.max_new_tokens,\n",
    "                    temperature=self.config.temperature,\n",
    "                    do_sample=True,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id\n",
    "                )\n",
    "                \n",
    "                prompt_len = inputs['input_ids'].shape[1]\n",
    "                response_ids = outputs.sequences[0, prompt_len:]\n",
    "                \n",
    "                # Compute log probs\n",
    "                log_probs = []\n",
    "                for i, score in enumerate(outputs.scores):\n",
    "                    if i >= len(response_ids):\n",
    "                        break\n",
    "                    probs = F.softmax(score[0] / self.config.temperature, dim=-1)\n",
    "                    log_prob = torch.log(probs[response_ids[i]] + 1e-10)\n",
    "                    log_probs.append(log_prob.item())\n",
    "                \n",
    "                # Reference log probs\n",
    "                ref_outputs = self.ref_policy(outputs.sequences)\n",
    "                ref_logits = ref_outputs.logits[0, prompt_len-1:-1]\n",
    "                ref_probs = F.softmax(ref_logits / self.config.temperature, dim=-1)\n",
    "                ref_log_probs = [\n",
    "                    torch.log(ref_probs[i, response_ids[i]] + 1e-10).item()\n",
    "                    for i in range(min(len(response_ids), len(ref_probs)))\n",
    "                ]\n",
    "                \n",
    "                # Compute reward\n",
    "                full_text = self.tokenizer.decode(outputs.sequences[0])\n",
    "                reward_input = self.tokenizer(full_text, return_tensors=\"pt\").to(self.reward_model.device)\n",
    "                reward = self.reward_model(reward_input['input_ids'], reward_input['attention_mask']).item()\n",
    "                \n",
    "                response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "            \n",
    "            batch_data.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": response,\n",
    "                \"log_probs\": log_probs,\n",
    "                \"ref_log_probs\": ref_log_probs,\n",
    "                \"reward\": reward\n",
    "            })\n",
    "        \n",
    "        # Compute advantages\n",
    "        rewards = []\n",
    "        advantages = []\n",
    "        for data in batch_data:\n",
    "            n_tokens = min(len(data[\"log_probs\"]), len(data[\"ref_log_probs\"]))\n",
    "            if n_tokens == 0:\n",
    "                continue\n",
    "                \n",
    "            token_rewards = []\n",
    "            for t in range(n_tokens):\n",
    "                kl_penalty = -self.kl_coef * (data[\"log_probs\"][t] - data[\"ref_log_probs\"][t])\n",
    "                r = kl_penalty\n",
    "                if t == n_tokens - 1:\n",
    "                    r += data[\"reward\"]\n",
    "                token_rewards.append(r)\n",
    "            \n",
    "            # Simple returns (no value function)\n",
    "            returns = []\n",
    "            cumulative = 0\n",
    "            for r in reversed(token_rewards):\n",
    "                cumulative = r + 0.99 * cumulative\n",
    "                returns.insert(0, cumulative)\n",
    "            \n",
    "            rewards.append(sum(token_rewards))\n",
    "            advantages.append(returns)\n",
    "        \n",
    "        # PPO update\n",
    "        self.policy.train()\n",
    "        total_loss = 0\n",
    "        total_kl = 0\n",
    "        \n",
    "        for _ in range(self.config.ppo_epochs):\n",
    "            for i, data in enumerate(batch_data):\n",
    "                if i >= len(advantages) or len(advantages[i]) == 0:\n",
    "                    continue\n",
    "                \n",
    "                formatted = f\"Question: {data['prompt']}\\n\\nAnswer: {data['response']}\"\n",
    "                inputs = self.tokenizer(formatted, return_tensors=\"pt\").to(self.policy.device)\n",
    "                \n",
    "                outputs = self.policy(**inputs)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                prompt_formatted = f\"Question: {data['prompt']}\\n\\nAnswer:\"\n",
    "                prompt_len = len(self.tokenizer(prompt_formatted)['input_ids'])\n",
    "                response_ids = inputs['input_ids'][0, prompt_len:]\n",
    "                \n",
    "                n_tokens = min(len(response_ids), len(data[\"log_probs\"]), len(advantages[i]))\n",
    "                if n_tokens == 0:\n",
    "                    continue\n",
    "                \n",
    "                new_log_probs = []\n",
    "                for t in range(n_tokens):\n",
    "                    probs = F.softmax(logits[0, prompt_len + t - 1] / self.config.temperature, dim=-1)\n",
    "                    lp = torch.log(probs[response_ids[t]] + 1e-10)\n",
    "                    new_log_probs.append(lp)\n",
    "                \n",
    "                new_log_probs = torch.stack(new_log_probs)\n",
    "                old_log_probs = torch.tensor(data[\"log_probs\"][:n_tokens], device=self.policy.device)\n",
    "                advs = torch.tensor(advantages[i][:n_tokens], device=self.policy.device, dtype=torch.float16)\n",
    "                \n",
    "                # Normalize\n",
    "                advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "                \n",
    "                # PPO clipped objective\n",
    "                ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "                pg_loss1 = -advs * ratio\n",
    "                pg_loss2 = -advs * torch.clamp(ratio, 1 - self.config.clip_range, 1 + self.config.clip_range)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                pg_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += pg_loss.item()\n",
    "                with torch.no_grad():\n",
    "                    total_kl += (old_log_probs - new_log_probs).mean().item()\n",
    "        \n",
    "        n_updates = len(batch_data) * self.config.ppo_epochs\n",
    "        avg_kl = total_kl / max(n_updates, 1)\n",
    "        \n",
    "        # Adaptive KL\n",
    "        if avg_kl > self.config.target_kl * 1.5:\n",
    "            self.kl_coef *= 1.2\n",
    "        elif avg_kl < self.config.target_kl / 1.5:\n",
    "            self.kl_coef /= 1.2\n",
    "        self.kl_coef = max(0.01, min(1.0, self.kl_coef))\n",
    "        \n",
    "        return {\n",
    "            \"mean_reward\": np.mean(rewards) if rewards else 0,\n",
    "            \"pg_loss\": total_loss / max(n_updates, 1),\n",
    "            \"kl\": avg_kl,\n",
    "            \"kl_coef\": self.kl_coef,\n",
    "            \"n_responses\": len(batch_data)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PPO trainer\n",
    "ppo_trainer = SimplePPOTrainer(\n",
    "    policy=model,\n",
    "    ref_policy=ref_policy,\n",
    "    reward_model=reward_model,\n",
    "    tokenizer=tokenizer,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Training prompts\n",
    "rl_prompts = TRAINING_PROMPTS * 3  # More prompts for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Training Loop\n",
    "print(\"Starting PPO training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "training_history = []\n",
    "\n",
    "for iteration in range(config.ppo_iterations):\n",
    "    # Sample batch\n",
    "    batch_prompts = random.sample(rl_prompts, min(config.ppo_batch_size, len(rl_prompts)))\n",
    "    \n",
    "    # Training step\n",
    "    metrics = ppo_trainer.train_step(batch_prompts)\n",
    "    training_history.append(metrics)\n",
    "    \n",
    "    # Logging\n",
    "    print(f\"Iteration {iteration+1}/{config.ppo_iterations}: \"\n",
    "          f\"Reward={metrics['mean_reward']:.3f}, \"\n",
    "          f\"KL={metrics['kl']:.4f}, \"\n",
    "          f\"Loss={metrics['pg_loss']:.4f}, \"\n",
    "          f\"KL_coef={metrics['kl_coef']:.4f}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PPO training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Reward\n",
    "rewards = [h['mean_reward'] for h in training_history]\n",
    "axes[0].plot(rewards)\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Mean Reward')\n",
    "axes[0].set_title('Reward During Training')\n",
    "\n",
    "# KL divergence\n",
    "kls = [h['kl'] for h in training_history]\n",
    "axes[1].plot(kls)\n",
    "axes[1].axhline(y=config.target_kl, color='r', linestyle='--', label='Target KL')\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('KL Divergence')\n",
    "axes[1].set_title('KL Divergence During Training')\n",
    "axes[1].legend()\n",
    "\n",
    "# Loss\n",
    "losses = [h['pg_loss'] for h in training_history]\n",
    "axes[2].plot(losses)\n",
    "axes[2].set_xlabel('Iteration')\n",
    "axes[2].set_ylabel('Policy Loss')\n",
    "axes[2].set_title('Policy Loss During Training')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare responses before and after training\n",
    "EVAL_PROMPTS = [\n",
    "    \"What is the difference between a list and a tuple in Python?\",\n",
    "    \"Explain how convolutional neural networks work.\",\n",
    "    \"What is the purpose of regularization?\",\n",
    "]\n",
    "\n",
    "print(\"COMPARISON: Reference vs Trained Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in EVAL_PROMPTS:\n",
    "    print(f\"\\nQuestion: {prompt}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Reference model\n",
    "    ref_response = generate_response(ref_policy, tokenizer, prompt)\n",
    "    print(f\"Reference: {ref_response[:200]}...\")\n",
    "    \n",
    "    # Trained model\n",
    "    trained_response = generate_response(model, tokenizer, prompt)\n",
    "    print(f\"Trained: {trained_response[:200]}...\")\n",
    "    \n",
    "    # Get rewards\n",
    "    with torch.no_grad():\n",
    "        ref_text = f\"Question: {prompt}\\n\\nAnswer: {ref_response}\"\n",
    "        ref_input = tokenizer(ref_text, return_tensors=\"pt\").to(reward_model.device)\n",
    "        ref_reward = reward_model(ref_input['input_ids']).item()\n",
    "        \n",
    "        train_text = f\"Question: {prompt}\\n\\nAnswer: {trained_response}\"\n",
    "        train_input = tokenizer(train_text, return_tensors=\"pt\").to(reward_model.device)\n",
    "        train_reward = reward_model(train_input['input_ids']).item()\n",
    "    \n",
    "    print(f\"\\nReward: Reference={ref_reward:.3f}, Trained={train_reward:.3f}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for reward hacking\n",
    "def check_reward_hacking(model, tokenizer, prompts):\n",
    "    \"\"\"Check for signs of reward hacking.\"\"\"\n",
    "    responses = [generate_response(model, tokenizer, p) for p in prompts]\n",
    "    \n",
    "    print(\"Reward Hacking Check\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Repetition\n",
    "    unique = set(responses)\n",
    "    repetition = 1 - len(unique) / len(responses)\n",
    "    print(f\"Repetition rate: {repetition:.1%}\")\n",
    "    if repetition > 0.3:\n",
    "        print(\"  ⚠️ High repetition - possible mode collapse\")\n",
    "    else:\n",
    "        print(\"  ✓ Low repetition\")\n",
    "    \n",
    "    # Length\n",
    "    lengths = [len(r) for r in responses]\n",
    "    print(f\"Average length: {np.mean(lengths):.0f} chars (std: {np.std(lengths):.0f})\")\n",
    "    if np.mean(lengths) > 300:\n",
    "        print(\"  ⚠️ Long responses - possible length gaming\")\n",
    "    else:\n",
    "        print(\"  ✓ Reasonable length\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    return responses\n",
    "\n",
    "test_prompts = [\n",
    "    \"What is a variable?\",\n",
    "    \"Explain functions.\",\n",
    "    \"What is a loop?\",\n",
    "    \"What is an array?\",\n",
    "    \"What is recursion?\",\n",
    "]\n",
    "\n",
    "_ = check_reward_hacking(model, tokenizer, test_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "**What you've built:**\n",
    "1. SFT model that understands Q&A format\n",
    "2. Reward model that predicts response quality\n",
    "3. PPO-trained policy optimized for the reward\n",
    "\n",
    "**Key insights:**\n",
    "- SFT teaches format; RL teaches quality\n",
    "- KL penalty prevents reward hacking\n",
    "- Reward models are imperfect proxies\n",
    "\n",
    "**Next steps:**\n",
    "- Try larger models (7B+)\n",
    "- Use real human preferences\n",
    "- Explore DPO as an alternative\n",
    "\n",
    "Learn more at [rlbook.ai/chapters/rlhf](https://rlbook.ai/chapters/rlhf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
