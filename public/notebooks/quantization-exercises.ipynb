{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quantization Exercises\n",
        "\n",
        "**From [rlbook.ai](https://rlbook.ai/chapters/quantization/summary-exercises)**\n",
        "\n",
        "Practice implementing quantization from scratch. Each exercise has:\n",
        "- A problem description\n",
        "- Starter code with TODOs\n",
        "- A solution (hidden by default)\n",
        "\n",
        "Try to solve each exercise before looking at the solution!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For Exercise 4\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 1: Implement Symmetric Quantization\n",
        "\n",
        "**Goal:** Implement functions that quantize float values to integers and recover them.\n",
        "\n",
        "**Background:**\n",
        "- Symmetric quantization maps the range `[-max, +max]` to `[-127, +127]` for int8\n",
        "- The scale factor is: `scale = max(|x|) / 127`\n",
        "- To quantize: `q = round(x / scale)`\n",
        "- To dequantize: `x_approx = q * scale`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1: Your code here\n",
        "\n",
        "def symmetric_quantize(x: np.ndarray, bits: int = 8) -> tuple[np.ndarray, float]:\n",
        "    \"\"\"\n",
        "    Quantize array x to signed integers with given bit width.\n",
        "\n",
        "    Args:\n",
        "        x: Input array of float values\n",
        "        bits: Number of bits (default 8)\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (quantized values as integers, scale factor)\n",
        "    \"\"\"\n",
        "    # TODO: Calculate qmax (maximum quantized value for signed int)\n",
        "    # Hint: For 8 bits, signed range is -128 to 127, so qmax = 127\n",
        "    qmax = None  # Your code here\n",
        "\n",
        "    # TODO: Calculate scale factor\n",
        "    # Hint: scale = max(|x|) / qmax\n",
        "    scale = None  # Your code here\n",
        "\n",
        "    # TODO: Quantize values\n",
        "    # Hint: round(x / scale), then clip to [-qmax, qmax], then cast to int8\n",
        "    q = None  # Your code here\n",
        "\n",
        "    return q, scale\n",
        "\n",
        "\n",
        "def symmetric_dequantize(q: np.ndarray, scale: float) -> np.ndarray:\n",
        "    \"\"\"Recover approximate float values from quantized integers.\"\"\"\n",
        "    # TODO: Multiply by scale to recover approximate values\n",
        "    # Hint: Cast q to float32 first\n",
        "    return None  # Your code here\n",
        "\n",
        "\n",
        "# Test your implementation\n",
        "weights = np.array([0.247, -0.103, 0.089, -0.156, 0.312, -0.278])\n",
        "q, scale = symmetric_quantize(weights, bits=8)\n",
        "recovered = symmetric_dequantize(q, scale)\n",
        "\n",
        "print(f\"Original:  {weights}\")\n",
        "print(f\"Quantized: {q}\")\n",
        "print(f\"Scale:     {scale:.6f}\")\n",
        "print(f\"Recovered: {recovered}\")\n",
        "print(f\"Max error: {np.abs(weights - recovered).max():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Solution\n",
        "\n",
        "Click to expand the solution after you've tried it yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Exercise 1: Solution (double-click to show)\n",
        "\n",
        "def symmetric_quantize_solution(x: np.ndarray, bits: int = 8) -> tuple[np.ndarray, float]:\n",
        "    \"\"\"Symmetric quantization to signed integers.\"\"\"\n",
        "    qmax = 2**(bits - 1) - 1  # 127 for int8\n",
        "    scale = np.max(np.abs(x)) / qmax\n",
        "    q = np.round(x / scale).clip(-qmax, qmax).astype(np.int8)\n",
        "    return q, scale\n",
        "\n",
        "\n",
        "def symmetric_dequantize_solution(q: np.ndarray, scale: float) -> np.ndarray:\n",
        "    \"\"\"Recover approximate float values from quantized integers.\"\"\"\n",
        "    return q.astype(np.float32) * scale\n",
        "\n",
        "\n",
        "# Test\n",
        "weights = np.array([0.247, -0.103, 0.089, -0.156, 0.312, -0.278])\n",
        "q, scale = symmetric_quantize_solution(weights, bits=8)\n",
        "recovered = symmetric_dequantize_solution(q, scale)\n",
        "\n",
        "print(f\"Original:  {weights}\")\n",
        "print(f\"Quantized: {q}\")  # Expected: [100, -42, 36, -63, 127, -113]\n",
        "print(f\"Scale:     {scale:.6f}\")  # Expected: ~0.002457\n",
        "print(f\"Recovered: {recovered}\")\n",
        "print(f\"Max error: {np.abs(weights - recovered).max():.6f}\")  # Expected: ~0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 2: Compare Per-Tensor vs Per-Channel\n",
        "\n",
        "**Goal:** Implement per-channel quantization and compare the error to per-tensor.\n",
        "\n",
        "**Background:**\n",
        "- Per-tensor: One scale for the entire weight matrix\n",
        "- Per-channel: One scale per output channel (row)\n",
        "- Per-channel is better when channels have different weight magnitudes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2: Your code here\n",
        "\n",
        "def per_tensor_quantize(weights: np.ndarray, bits: int = 8) -> tuple[np.ndarray, float]:\n",
        "    \"\"\"Quantize entire tensor with single scale (provided for reference).\"\"\"\n",
        "    qmax = 2**(bits - 1) - 1\n",
        "    scale = np.max(np.abs(weights)) / qmax\n",
        "    q = np.round(weights / scale).clip(-qmax, qmax).astype(np.int8)\n",
        "    return q, scale\n",
        "\n",
        "\n",
        "def per_channel_quantize(weights: np.ndarray, bits: int = 8) -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Quantize each row (output channel) with its own scale.\n",
        "\n",
        "    Args:\n",
        "        weights: 2D array of shape (out_channels, in_channels)\n",
        "        bits: Number of bits\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (quantized values, array of scales per channel)\n",
        "    \"\"\"\n",
        "    qmax = 2**(bits - 1) - 1\n",
        "\n",
        "    # TODO: Calculate per-row scales (one scale per output channel)\n",
        "    # Hint: Use np.max with axis=1 to get max per row\n",
        "    scales = None  # Your code here\n",
        "\n",
        "    # TODO: Quantize each row with its scale\n",
        "    # Hint: Use broadcasting with scales[:, None] to divide each row by its scale\n",
        "    q = None  # Your code here\n",
        "\n",
        "    return q, scales\n",
        "\n",
        "\n",
        "# Create weights with very different magnitudes per channel\n",
        "np.random.seed(42)\n",
        "weights = np.random.randn(4, 64)\n",
        "weights[0] *= 0.01   # Tiny weights\n",
        "weights[1] *= 0.1    # Small weights\n",
        "weights[2] *= 1.0    # Normal weights\n",
        "weights[3] *= 10.0   # Large weights\n",
        "\n",
        "print(\"Weight magnitudes per channel:\")\n",
        "for i in range(4):\n",
        "    print(f\"  Channel {i}: max = {np.abs(weights[i]).max():.4f}\")\n",
        "\n",
        "# Quantize both ways\n",
        "q_tensor, scale_tensor = per_tensor_quantize(weights)\n",
        "q_channel, scales_channel = per_channel_quantize(weights)\n",
        "\n",
        "# TODO: Calculate reconstruction errors\n",
        "# Hint: For per-tensor, multiply all by scale_tensor\n",
        "# Hint: For per-channel, multiply each row by its scale (use scales_channel[:, None])\n",
        "recovered_tensor = None  # Your code here\n",
        "recovered_channel = None  # Your code here\n",
        "\n",
        "mse_tensor = np.mean((weights - recovered_tensor)**2)\n",
        "mse_channel = np.mean((weights - recovered_channel)**2)\n",
        "\n",
        "print(f\"\\nPer-tensor MSE:  {mse_tensor:.8f}\")\n",
        "print(f\"Per-channel MSE: {mse_channel:.8f}\")\n",
        "print(f\"Improvement:     {mse_tensor / mse_channel:.1f}x better\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Exercise 2: Solution (double-click to show)\n",
        "\n",
        "def per_tensor_quantize_solution(weights: np.ndarray, bits: int = 8) -> tuple[np.ndarray, float]:\n",
        "    qmax = 2**(bits - 1) - 1\n",
        "    scale = np.max(np.abs(weights)) / qmax\n",
        "    q = np.round(weights / scale).clip(-qmax, qmax).astype(np.int8)\n",
        "    return q, scale\n",
        "\n",
        "\n",
        "def per_channel_quantize_solution(weights: np.ndarray, bits: int = 8) -> tuple[np.ndarray, np.ndarray]:\n",
        "    qmax = 2**(bits - 1) - 1\n",
        "    # One scale per row\n",
        "    scales = np.max(np.abs(weights), axis=1) / qmax\n",
        "    # Quantize each row with its own scale\n",
        "    q = np.round(weights / scales[:, None]).clip(-qmax, qmax).astype(np.int8)\n",
        "    return q, scales\n",
        "\n",
        "\n",
        "# Create weights with very different magnitudes per channel\n",
        "np.random.seed(42)\n",
        "weights = np.random.randn(4, 64)\n",
        "weights[0] *= 0.01\n",
        "weights[1] *= 0.1\n",
        "weights[2] *= 1.0\n",
        "weights[3] *= 10.0\n",
        "\n",
        "# Quantize both ways\n",
        "q_tensor, scale_tensor = per_tensor_quantize_solution(weights)\n",
        "q_channel, scales_channel = per_channel_quantize_solution(weights)\n",
        "\n",
        "# Reconstruct\n",
        "recovered_tensor = q_tensor.astype(np.float32) * scale_tensor\n",
        "recovered_channel = q_channel.astype(np.float32) * scales_channel[:, None]\n",
        "\n",
        "# Compare errors\n",
        "mse_tensor = np.mean((weights - recovered_tensor)**2)\n",
        "mse_channel = np.mean((weights - recovered_channel)**2)\n",
        "\n",
        "print(f\"Per-tensor MSE:  {mse_tensor:.8f}\")\n",
        "print(f\"Per-channel MSE: {mse_channel:.8f}\")\n",
        "print(f\"Improvement:     {mse_tensor / mse_channel:.1f}x better\")\n",
        "\n",
        "# Per-channel is ~100x better when channel scales vary widely!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 3: Visualize Quantization Error vs Bit Width\n",
        "\n",
        "**Goal:** Create a plot showing how quantization error decreases as you use more bits.\n",
        "\n",
        "**Background:**\n",
        "- More bits = more discrete levels = less error\n",
        "- Each additional bit roughly doubles the number of levels\n",
        "- Error decreases exponentially (roughly 4x per bit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 3: Your code here\n",
        "\n",
        "def quantize_and_measure_error(x: np.ndarray, bits: int) -> float:\n",
        "    \"\"\"Quantize x and return the mean squared error.\"\"\"\n",
        "    qmax = 2**(bits - 1) - 1\n",
        "    scale = np.max(np.abs(x)) / qmax\n",
        "    q = np.round(x / scale).clip(-qmax, qmax)\n",
        "    recovered = q * scale\n",
        "    return np.mean((x - recovered)**2)\n",
        "\n",
        "\n",
        "# Generate random weights\n",
        "np.random.seed(42)\n",
        "weights = np.random.randn(1000)\n",
        "\n",
        "# TODO: Test different bit widths (2, 3, 4, 5, 6, 7, 8)\n",
        "bit_widths = [2, 3, 4, 5, 6, 7, 8]\n",
        "errors = None  # Your code here: list of errors for each bit width\n",
        "\n",
        "# TODO: Create the plot\n",
        "# Hint: Use plt.semilogy() for log y-axis\n",
        "# Hint: Add labels, title, grid\n",
        "\n",
        "# Your plotting code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Exercise 3: Solution (double-click to show)\n",
        "\n",
        "def quantize_and_measure_error_solution(x: np.ndarray, bits: int) -> float:\n",
        "    qmax = 2**(bits - 1) - 1\n",
        "    scale = np.max(np.abs(x)) / qmax\n",
        "    q = np.round(x / scale).clip(-qmax, qmax)\n",
        "    recovered = q * scale\n",
        "    return np.mean((x - recovered)**2)\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "weights = np.random.randn(1000)\n",
        "\n",
        "bit_widths = [2, 3, 4, 5, 6, 7, 8]\n",
        "errors = [quantize_and_measure_error_solution(weights, b) for b in bit_widths]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogy(bit_widths, errors, 'o-', markersize=10, linewidth=2, color='#06b6d4')\n",
        "plt.xlabel('Bits', fontsize=12)\n",
        "plt.ylabel('Mean Squared Error (log scale)', fontsize=12)\n",
        "plt.title('Quantization Error vs Bit Width', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(bit_widths)\n",
        "\n",
        "# Add annotations\n",
        "for b, e in zip(bit_widths, errors):\n",
        "    plt.annotate(f'{e:.2e}', (b, e), textcoords=\"offset points\",\n",
        "                 xytext=(0, 10), ha='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print the reduction factor\n",
        "print(\"\\nError reduction per additional bit:\")\n",
        "for i in range(1, len(errors)):\n",
        "    print(f\"  {bit_widths[i-1]} â†’ {bit_widths[i]} bits: {errors[i-1]/errors[i]:.1f}x reduction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Exercise 4: Quantize a PyTorch Model\n",
        "\n",
        "**Goal:** Use PyTorch's built-in quantization and measure the benefits.\n",
        "\n",
        "**Background:**\n",
        "- `torch.quantization.quantize_dynamic` does post-training quantization\n",
        "- It quantizes Linear layers to int8\n",
        "- Benefits: smaller model size, faster inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 4: Your code here\n",
        "import time\n",
        "from io import BytesIO\n",
        "\n",
        "# Simple model\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "\n",
        "# Create model\n",
        "model = SimpleNet()\n",
        "model.eval()\n",
        "\n",
        "# TODO: Use torch.quantization.quantize_dynamic to quantize the model\n",
        "# Hint: quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
        "quantized_model = None  # Your code here\n",
        "\n",
        "\n",
        "# TODO: Compare model sizes\n",
        "# Hint: Save to BytesIO buffer, check buffer.tell() for size\n",
        "def get_model_size(m):\n",
        "    buffer = BytesIO()\n",
        "    torch.save(m.state_dict(), buffer)\n",
        "    return buffer.tell()\n",
        "\n",
        "original_size = get_model_size(model)\n",
        "quantized_size = None  # Your code here\n",
        "\n",
        "print(f\"Original size:  {original_size / 1024:.1f} KB\")\n",
        "print(f\"Quantized size: {quantized_size / 1024:.1f} KB\")\n",
        "print(f\"Compression:    {original_size / quantized_size:.1f}x\")\n",
        "\n",
        "\n",
        "# TODO: Compare inference speed\n",
        "# Hint: Time 1000 forward passes with random input\n",
        "x = torch.randn(1, 784)\n",
        "n_runs = 1000\n",
        "\n",
        "# Warmup\n",
        "for _ in range(100):\n",
        "    _ = model(x)\n",
        "    _ = quantized_model(x)\n",
        "\n",
        "# Your timing code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4: Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Exercise 4: Solution (double-click to show)\n",
        "import time\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "\n",
        "model = SimpleNet()\n",
        "model.eval()\n",
        "\n",
        "# Dynamic quantization\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Compare sizes\n",
        "def get_model_size(m):\n",
        "    buffer = BytesIO()\n",
        "    torch.save(m.state_dict(), buffer)\n",
        "    return buffer.tell()\n",
        "\n",
        "original_size = get_model_size(model)\n",
        "quantized_size = get_model_size(quantized_model)\n",
        "\n",
        "print(f\"Original size:  {original_size / 1024:.1f} KB\")\n",
        "print(f\"Quantized size: {quantized_size / 1024:.1f} KB\")\n",
        "print(f\"Compression:    {original_size / quantized_size:.1f}x\")\n",
        "\n",
        "# Compare speed\n",
        "x = torch.randn(1, 784)\n",
        "n_runs = 1000\n",
        "\n",
        "# Warmup\n",
        "for _ in range(100):\n",
        "    _ = model(x)\n",
        "    _ = quantized_model(x)\n",
        "\n",
        "# Time original\n",
        "start = time.time()\n",
        "for _ in range(n_runs):\n",
        "    _ = model(x)\n",
        "original_time = time.time() - start\n",
        "\n",
        "# Time quantized\n",
        "start = time.time()\n",
        "for _ in range(n_runs):\n",
        "    _ = quantized_model(x)\n",
        "quantized_time = time.time() - start\n",
        "\n",
        "print(f\"\\nOriginal time:  {original_time*1000:.1f} ms for {n_runs} runs\")\n",
        "print(f\"Quantized time: {quantized_time*1000:.1f} ms for {n_runs} runs\")\n",
        "print(f\"Speedup:        {original_time/quantized_time:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "In these exercises, you learned:\n",
        "\n",
        "1. **Symmetric quantization**: Map floats to integers using a scale factor\n",
        "2. **Per-channel vs per-tensor**: Per-channel is much better when weights vary across channels\n",
        "3. **Error vs bits**: Each additional bit roughly halves the quantization error\n",
        "4. **PyTorch quantization**: `quantize_dynamic` gives ~3-4x size reduction with minimal code\n",
        "\n",
        "**Next steps:**\n",
        "- Try quantizing a larger model (ResNet, BERT)\n",
        "- Experiment with static quantization (requires calibration data)\n",
        "- Explore quantization-aware training for better accuracy"
      ]
    }
  ]
}
