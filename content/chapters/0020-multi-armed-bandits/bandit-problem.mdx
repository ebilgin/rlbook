---
title: "The Bandit Problem"
description: "A simplified RL setting for understanding exploration"
---

import { Intuition, Mathematical, Implementation, DeepDive } from '@/components/ui/ContentLayers';
import { Note, Warning, Tip } from '@/components/ui/Callouts';

## The Casino Problem

Imagine you're in a casino with 10 slot machines. Each machine has a different (unknown) payout rate. You have 1000 coins to spend. How do you maximize your winnings?

You could stick with the first machine that seems good—but what if there's a better one you never tried? You could try every machine equally—but then you're wasting pulls on bad machines you've already identified.

This is the **multi-armed bandit problem**—named after the old term "one-armed bandit" for slot machines. It's the purest form of the exploration-exploitation dilemma, stripped of any complexity about states or sequences.

<Intuition>

The bandit problem gives us a laboratory for studying exploration:

- There are no **states**—the situation doesn't change based on your actions
- There's no **sequence**—each pull is independent
- There's only one decision: **which arm to pull**

By removing these complexities, we can focus entirely on the question: *How do we learn which option is best while also maximizing reward along the way?*

Every technique we develop here—ε-greedy, UCB, Thompson Sampling—will reappear when we tackle full RL problems.

</Intuition>

## The Formal Setup

<Mathematical>

A **k-armed bandit** is defined by:

- **k actions** (arms): $a \in \{1, 2, \ldots, k\}$
- **Reward distributions**: Each arm $a$ has an unknown distribution with mean $\mu_a$
- **Goal**: Maximize total reward over $T$ pulls

When you pull arm $a$, you receive a random reward $R$ drawn from that arm's distribution. The expected reward is $\mu_a$, but you don't know it—you have to learn it from experience.

The **optimal arm** is $a^* = \arg\max_a \mu_a$ with expected reward $\mu^*$.

</Mathematical>
