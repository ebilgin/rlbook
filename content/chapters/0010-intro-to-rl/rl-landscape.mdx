---
title: "The RL Landscape"
description: "Model-free vs model-based, value vs policy methods"
---

import { Intuition, Mathematical, Implementation, DeepDive } from '@/components/ui/ContentLayers';
import { ChapterObjectives, KeyTakeaways, NextChapter, CrossRef } from '@/components/ui/ChapterNav';
import { Note, Warning, Tip } from '@/components/ui/Callouts';

## How This Book Is Organized

Here's your roadmap:

<Intuition>

**Part 1: Foundations**
- Introduction to RL (you are here)
- Multi-Armed Bandits (simplest RL: no states)
- Contextual Bandits (adding context features)
- Markov Decision Processes (formalizing sequential decisions)

**Part 2: Value-Based Methods**
- Dynamic Programming (with full environment knowledge)
- Monte Carlo Methods (learning from complete episodes)
- Temporal Difference Learning (learning step-by-step)
- Q-Learning (the foundation of modern RL)

**Part 3: Deep Reinforcement Learning**
- Deep Q-Networks (Q-learning with neural networks)
- Policy Gradient Methods (learning policies directly)
- Actor-Critic Architectures (combining value and policy)
- Advanced Topics (curiosity, multi-agent, etc.)

</Intuition>

Each chapter builds on previous ones. We'll start simple and add complexity gradually.

## Summary

<KeyTakeaways>
- **Reinforcement learning** is learning from interaction: take actions, observe rewards, improve behavior.
- The RL loop: Agent observes **state** → chooses **action** → receives **reward** → new state → repeat.
- Unlike supervised learning, RL learns from **delayed, sparse, evaluative feedback**, not correct labels.
- The goal is to maximize **cumulative reward** over time, not just immediate reward.
- The **exploration-exploitation tradeoff** is fundamental: exploit what you know vs. explore to learn more.
- RL powers game-playing AI, robotics, recommendations, and more.
</KeyTakeaways>

Now that we have the big picture, let's start with the simplest possible RL problem: choosing between options with uncertain rewards. This is the multi-armed bandit problem, and it's where we'll learn the fundamental techniques that scale to complex environments.

<NextChapter slug="multi-armed-bandits" title="Multi-Armed Bandits" />

## Exercises

### Conceptual Questions

1. **List three everyday examples of reinforcement learning** (human or animal, not computer). For each, identify the agent, environment, states, actions, and rewards.

2. **Why can't we use supervised learning for game playing?** What specific challenges does RL address that supervised learning cannot?

3. **What's the risk if an agent only exploits and never explores?** Give a concrete example where this would lead to poor performance.

### Think About It

4. **Frame a problem you face as RL.** Think of a repeated decision you make (commute route, workout routine, study schedule). What would be the state, actions, and rewards? What makes it hard to optimize?

5. **When is RL overkill?** Some problems don't need RL—supervised learning or even simple rules work fine. What characteristics of a problem make RL the right choice?
