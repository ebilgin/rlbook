---
title: "Rewards and Returns"
description: "Defining goals through reward signals"
---

import { Intuition, Mathematical, Implementation, DeepDive } from '@/components/ui/ContentLayers';
import { Note, Warning, Tip } from '@/components/ui/Callouts';

## The Goal: Maximize Cumulative Reward

The agent's objective isn't to maximize immediate reward—it's to maximize **cumulative reward** over time, also called the **return**.

<Intuition>

This distinction is crucial. A move in chess that captures a pawn might look good now, but if it leads to losing your queen, the cumulative reward is negative.

RL agents must learn to delay gratification. Sometimes the best immediate action is to invest in future rewards.

</Intuition>

<Mathematical>

Formally, the agent tries to maximize the expected return:

$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + \ldots$$

where $R_{t+1}$ is the reward received after time step $t$.

Often we add **discounting**—valuing immediate rewards more than distant ones:

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots$$

where $\gamma$ (gamma) is the discount factor, typically between 0.9 and 0.999.

</Mathematical>

## The Exploration-Exploitation Dilemma

Here's a fundamental challenge every RL agent faces.

<Intuition>

Imagine you're in a new city looking for a good restaurant. You've found a decent place that you know serves acceptable food. Do you:

- **Exploit**: Go back to the known restaurant. You're guaranteed an acceptable meal.
- **Explore**: Try a new place. It might be amazing—or terrible.

This is the **exploration-exploitation tradeoff**:

- **Exploitation**: Use what you know to get reward now
- **Exploration**: Try new things to learn more for the future

Too much exploitation and you get stuck with mediocrity. Too much exploration and you waste time on bad options. The art of RL is balancing these forces.

</Intuition>

We'll develop strategies for this balance throughout the book, starting with simple approaches in the next chapter on multi-armed bandits.
