---
title: "RL for Language Models"
slug: "rl-for-llms"
section: "Advanced Topics"
description: "From RLHF to reasoning: how RL transforms language models"
status: "draft"
lastReviewed: null
prerequisites:
  - slug: "reinforce"
    title: "REINFORCE"
  - slug: "ppo"
    title: "Proximal Policy Optimization"
---

import { Intuition, Mathematical, Implementation } from '@/components/ui/ContentLayers';
import { ChapterObjectives, KeyTakeaways, NextChapter } from '@/components/ui/ChapterNav';
import { Note, Warning, Tip, Example, Definition } from '@/components/ui/Callouts';
import TrainingStagePipeline from '@/components/interactive/TrainingStagePipeline';

# RL for Language Models

<ChapterObjectives>
- Understand the two motivations for RL in LLMs: alignment and reasoning
- Build a reward model from human preferences using the Bradley-Terry model
- Compare PPO, DPO, and GRPO — know when to use each
- Explain how GRPO with verifiable rewards produces reasoning models
- Walk through a real GRPO implementation (Karpathy's nanochat)
- Identify open challenges: reward hacking, mode collapse, scalable oversight
</ChapterObjectives>

Every modern LLM goes through multiple training stages. Each stage transforms how the model responds. Click through the pipeline to see the difference:

<TrainingStagePipeline client:load />

<Intuition>

**What you just saw:** The same model, transformed step by step. Pretraining teaches it language. Supervised fine-tuning teaches it to follow instructions. RL teaches it what humans actually want — and, more recently, how to think through hard problems.

</Intuition>

## Chapter Overview

<div className="my-8">
  <div className="text-slate-400 text-sm mb-4">In this chapter:</div>

  <div className="space-y-3">
    <a href="/chapters/rl-for-llms/why-rl-for-llms" className="block bg-gradient-to-r from-slate-800/50 to-transparent rounded-xl p-5 border-l-4 border-cyan-500 hover:from-slate-700/50 transition-colors">
      <div className="text-cyan-400 font-bold text-lg">Why RL for Language Models?</div>
      <div className="text-slate-400 text-sm mt-1">The alignment problem, the reasoning revolution, and why supervised learning isn't enough</div>
    </a>

    <a href="/chapters/rl-for-llms/reward-modeling" className="block bg-gradient-to-r from-slate-800/50 to-transparent rounded-xl p-5 border-l-4 border-violet-500 hover:from-slate-700/50 transition-colors">
      <div className="text-violet-400 font-bold text-lg">Reward Modeling</div>
      <div className="text-slate-400 text-sm mt-1">Learning what humans want from pairwise preferences using the Bradley-Terry model</div>
    </a>

    <a href="/chapters/rl-for-llms/rl-algorithms-for-llms" className="block bg-gradient-to-r from-slate-800/50 to-transparent rounded-xl p-5 border-l-4 border-blue-500 hover:from-slate-700/50 transition-colors">
      <div className="text-blue-400 font-bold text-lg">RL Algorithms for LLMs</div>
      <div className="text-slate-400 text-sm mt-1">PPO, DPO, and GRPO — the evolution from complex to simple, and when to use each</div>
    </a>

    <a href="/chapters/rl-for-llms/grpo-and-reasoning" className="block bg-gradient-to-r from-slate-800/50 to-transparent rounded-xl p-5 border-l-4 border-amber-500 hover:from-slate-700/50 transition-colors">
      <div className="text-amber-400 font-bold text-lg">GRPO and the Reasoning Revolution</div>
      <div className="text-slate-400 text-sm mt-1">Verifiable rewards, DeepSeek R1, and how RL produces chain-of-thought reasoning</div>
    </a>

    <a href="/chapters/rl-for-llms/hands-on" className="block bg-gradient-to-r from-slate-800/50 to-transparent rounded-xl p-5 border-l-4 border-emerald-500 hover:from-slate-700/50 transition-colors">
      <div className="text-emerald-400 font-bold text-lg">Building a Reasoning Model</div>
      <div className="text-slate-400 text-sm mt-1">Hands-on with Karpathy's nanochat implementation and simplified GRPO</div>
    </a>

    <a href="/chapters/rl-for-llms/challenges" className="block bg-gradient-to-r from-slate-800/50 to-transparent rounded-xl p-5 border-l-4 border-red-500 hover:from-slate-700/50 transition-colors">
      <div className="text-red-400 font-bold text-lg">Challenges and Frontiers</div>
      <div className="text-slate-400 text-sm mt-1">Reward hacking, mode collapse, Constitutional AI, and what's next</div>
    </a>
  </div>
</div>

<Note title="The Journey Comes Full Circle">

Everything in this book leads here. [Policy gradients](/chapters/intro-to-policy-gradients) taught us how to optimize policies directly — LLMs are policies over tokens. [REINFORCE](/chapters/reinforce) gave us the foundation — GRPO is essentially REINFORCE with group baselines. [Actor-Critic methods](/chapters/actor-critic) introduced the advantage function — GRPO replaces the learned critic with group statistics. [PPO](/chapters/ppo) provided the clipped objective that makes RLHF stable.

RL for language models is where these techniques become a transformative technology.

</Note>

<Tip title="Start Here">

Begin with [Why RL for Language Models?](/chapters/rl-for-llms/why-rl-for-llms) to understand both motivations — alignment and reasoning — and how the pieces fit together.

</Tip>

---

<KeyTakeaways>
- RL for LLMs has two motivations: alignment (what humans want) and reasoning (developing capabilities)
- RLHF uses human preferences to train a reward model, then optimizes with PPO
- GRPO eliminates the critic network and trains with verifiable rewards instead
- The algorithm evolution — PPO to DPO to GRPO — trends toward simplicity and scalability
- DeepSeek R1 showed that RL can produce emergent reasoning capabilities
- Open challenges include reward hacking, mode collapse, and aligning systems smarter than us
</KeyTakeaways>
