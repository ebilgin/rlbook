---
title: "The Bellman Equations"
slug: "bellman-equations"
section: "Markov Decision Processes"
description: "The recursive equations that make RL possible"
status: "draft"
lastReviewed: null
prerequisites:
  - slug: "intro-to-mdps"
    title: "Introduction to MDPs"
  - slug: "value-functions"
    title: "Value Functions"
---

import { Intuition, Mathematical, Implementation } from '@/components/ui/ContentLayers';
import { ChapterObjectives, KeyTakeaways, NextChapter } from '@/components/ui/ChapterNav';
import { Note, Warning, Tip, Example, Definition } from '@/components/ui/Callouts';

# The Bellman Equations

<ChapterObjectives>
- Derive the Bellman expectation equation for $V^\pi$ and $Q^\pi$
- Derive the Bellman optimality equations for $V^*$ and $Q^*$
- Explain what "bootstrapping" means and why it enables efficient learning
- Understand why Bellman equations are the foundation of all RL algorithms
</ChapterObjectives>

Richard Bellman discovered something beautiful in the 1950s: **the value of a state depends on the values of states you can reach from it.** This simple recursive insight---that you can break down a complex problem into simpler subproblems---is the foundation of everything in reinforcement learning.

## The Recursive Nature of Value

<Intuition>

Consider standing at a crossroads. How valuable is your current position? It depends on:

1. **What reward you get right now** (maybe there's a rest stop here)
2. **Where you can go from here** (the available roads)
3. **How valuable those destinations are** (what awaits down each path)

But wait---the value of those destinations depends on *their* successors. And those depend on further successors. It seems like an infinite regress!

Bellman's insight was that this recursion is actually a *feature*, not a bug. The value of any state can be expressed in terms of immediate rewards plus the values of neighboring states. This creates a system of equations that we can solve.

</Intuition>

<Definition title="Bellman Equations">
Recursive relationships that express the value of a state (or state-action pair) in terms of immediate rewards and the values of successor states. They form a system of equations that value functions must satisfy.
</Definition>

## The Core Insight: Value Flows Backward

Think about how you might evaluate different starting positions in a maze:

<Example title="A Simple Maze">

Consider a 3-state path: Start, Middle, Goal

<div className="my-6 flex justify-center">
  <div className="inline-flex items-center gap-4">
    <div className="w-16 h-16 bg-blue-600/40 border border-slate-600 rounded-lg flex items-center justify-center text-lg font-semibold">Start</div>
    <div className="text-slate-400 text-2xl">&rarr;</div>
    <div className="w-16 h-16 bg-slate-700/50 border border-slate-600 rounded-lg flex items-center justify-center text-lg font-semibold">Mid</div>
    <div className="text-slate-400 text-2xl">&rarr;</div>
    <div className="w-16 h-16 bg-emerald-600/40 border border-slate-600 rounded-lg flex items-center justify-center text-lg">+10</div>
  </div>
</div>

- **Goal value**: $V(\text{Goal}) = 10$ (the reward)
- **Middle value**: $V(\text{Mid}) = 0 + \gamma \cdot V(\text{Goal}) = 0 + 0.9 \cdot 10 = 9$
- **Start value**: $V(\text{Start}) = 0 + \gamma \cdot V(\text{Mid}) = 0 + 0.9 \cdot 9 = 8.1$

Value flows *backward* from rewards through the state space.

</Example>

This backward flow is the essence of the Bellman equations. Each state's value depends on its successors' values, which depend on *their* successors' values, all the way to the rewards.

## Chapter Overview

This chapter explores the mathematical heart of reinforcement learning---the Bellman equations:

<div className="grid md:grid-cols-3 gap-4 my-6">
  <a href="/chapters/bellman-equations/bellman-expectation" className="block p-4 bg-gradient-to-br from-cyan-900/30 to-cyan-800/10 rounded-lg border border-cyan-700/50 hover:border-cyan-500/50 transition-colors">
    <h3 className="font-semibold text-cyan-400">Bellman Expectation Equations</h3>
    <p className="text-slate-400 text-sm mt-1">How values relate under a specific policy</p>
  </a>
  <a href="/chapters/bellman-equations/bellman-optimality" className="block p-4 bg-gradient-to-br from-violet-900/30 to-violet-800/10 rounded-lg border border-violet-700/50 hover:border-violet-500/50 transition-colors">
    <h3 className="font-semibold text-violet-400">Bellman Optimality Equations</h3>
    <p className="text-slate-400 text-sm mt-1">The equations that define optimal behavior</p>
  </a>
  <a href="/chapters/bellman-equations/why-bellman-matters" className="block p-4 bg-gradient-to-br from-amber-900/30 to-amber-800/10 rounded-lg border border-amber-700/50 hover:border-amber-500/50 transition-colors">
    <h3 className="font-semibold text-amber-400">Why Bellman Matters</h3>
    <p className="text-slate-400 text-sm mt-1">Bootstrapping and the foundation of RL</p>
  </a>
</div>

## The Two Types of Bellman Equations

There are two families of Bellman equations, and understanding the difference is crucial:

### Expectation Equations

The **Bellman expectation equations** describe values under a *specific* policy $\pi$. They answer: "If I follow this particular strategy, how good is each state?"

$$V^\pi(s) = \sum_{a} \pi(a|s) \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^\pi(s') \right]$$

The key feature: we *average* over actions according to the policy $\pi(a|s)$.

### Optimality Equations

The **Bellman optimality equations** describe the *best possible* values. They answer: "If I act optimally from now on, how good is each state?"

$$V^*(s) = \max_a \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^*(s') \right]$$

The key feature: we take the *maximum* over actions, not the average.

<Note>
The shift from $\sum_a \pi(a|s)$ to $\max_a$ is subtle but profound. It means the optimality equations don't require knowing the optimal policy---the $\max$ operator embeds the optimal action choice directly into the equation.
</Note>

## Why This Chapter Matters

The Bellman equations are not just theoretical curiosities---they're the foundation of every major RL algorithm:

<div className="my-6 space-y-3">
  <div className="p-3 bg-slate-800/50 rounded-lg border border-slate-700">
    <span className="font-semibold text-cyan-400">Dynamic Programming:</span>
    <span className="text-slate-400 ml-2">Solves Bellman equations exactly when model is known</span>
  </div>
  <div className="p-3 bg-slate-800/50 rounded-lg border border-slate-700">
    <span className="font-semibold text-violet-400">Monte Carlo:</span>
    <span className="text-slate-400 ml-2">Estimates values by sampling complete returns</span>
  </div>
  <div className="p-3 bg-slate-800/50 rounded-lg border border-slate-700">
    <span className="font-semibold text-amber-400">Temporal Difference:</span>
    <span className="text-slate-400 ml-2">Approximates Bellman with single-step sampled updates</span>
  </div>
  <div className="p-3 bg-slate-800/50 rounded-lg border border-slate-700">
    <span className="font-semibold text-emerald-400">Q-Learning:</span>
    <span className="text-slate-400 ml-2">Learns optimal values via sampled Bellman optimality updates</span>
  </div>
</div>

<Tip>
If you understand the Bellman equations deeply, you understand the core of reinforcement learning. Everything else is about how to solve or approximate these equations under different conditions---when you know the model, when you don't, when states are discrete, when they're continuous.
</Tip>

## The Journey Ahead

In this chapter, we'll build up the Bellman equations step by step:

1. **Bellman Expectation**: We'll derive the equations from the definition of return, showing exactly how today's value depends on tomorrow's value.

2. **Bellman Optimality**: We'll see how replacing the policy average with a maximum gives us equations that define optimal behavior without needing to specify an optimal policy.

3. **Why It Matters**: We'll explore bootstrapping---the key insight that lets us update estimates using other estimates---and see why this makes RL algorithms possible.

<Mathematical>

By the end of this chapter, you'll be comfortable with all four Bellman equations:

**For state values:**
- Expectation: $V^\pi(s) = \sum_{a} \pi(a|s) \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^\pi(s') \right]$
- Optimality: $V^*(s) = \max_a \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^*(s') \right]$

**For action values:**
- Expectation: $Q^\pi(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) \sum_{a'} \pi(a'|s') Q^\pi(s',a')$
- Optimality: $Q^*(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) \max_{a'} Q^*(s',a')$

</Mathematical>

---

<KeyTakeaways>
- Bellman equations express values recursively in terms of successor values
- **Expectation equations** average over the policy: $\sum_a \pi(a|s) [\ldots]$
- **Optimality equations** maximize over actions: $\max_a [\ldots]$
- Value flows backward from rewards through the state space
- These equations are the foundation of all RL algorithms
</KeyTakeaways>

<NextChapter slug="policy-evaluation" title="Policy Evaluation" />
