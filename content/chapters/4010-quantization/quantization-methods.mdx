---
title: "Quantization Methods"
slug: "quantization-methods"
section: "ML Concepts"
description: "Post-training quantization vs quantization-aware training"
---

import { Intuition, Mathematical, Implementation } from '@/components/ui/ContentLayers';
import { Note, Warning, Tip, Example, Definition } from '@/components/ui/Callouts';

# Quantization Methods

You know the "what" of quantization. Now let's cover the "how"‚Äîthe methods for actually quantizing a model.

## The Two Approaches

<div className="grid md:grid-cols-2 gap-4 my-8">
  <div className="bg-gradient-to-br from-cyan-900/30 to-cyan-800/10 rounded-xl p-6 border border-cyan-700/50">
    <div className="text-3xl mb-3">üì¶</div>
    <div className="text-cyan-400 font-bold text-xl mb-2">Post-Training (PTQ)</div>
    <div className="text-slate-400 text-sm mb-4">Quantize after training is complete</div>
    <div className="space-y-2 text-sm">
      <div className="flex items-center gap-2"><span className="text-emerald-400">‚úì</span> Fast‚Äîno retraining</div>
      <div className="flex items-center gap-2"><span className="text-emerald-400">‚úì</span> Works for huge models</div>
      <div className="flex items-center gap-2"><span className="text-amber-400">~</span> May lose some accuracy</div>
    </div>
  </div>
  <div className="bg-gradient-to-br from-violet-900/30 to-violet-800/10 rounded-xl p-6 border border-violet-700/50">
    <div className="text-3xl mb-3">üîß</div>
    <div className="text-violet-400 font-bold text-xl mb-2">Quantization-Aware (QAT)</div>
    <div className="text-slate-400 text-sm mb-4">Train with quantization in the loop</div>
    <div className="space-y-2 text-sm">
      <div className="flex items-center gap-2"><span className="text-emerald-400">‚úì</span> Best accuracy</div>
      <div className="flex items-center gap-2"><span className="text-emerald-400">‚úì</span> Works for extreme quantization</div>
      <div className="flex items-center gap-2"><span className="text-red-400">‚úó</span> Requires training compute</div>
    </div>
  </div>
</div>

## Post-Training Quantization (PTQ)

PTQ is simple: take a trained model, convert weights to lower precision.

<div className="my-8 p-6 bg-slate-800/30 rounded-xl border border-slate-700">
  <div className="flex items-center justify-center gap-4 flex-wrap">
    <div className="text-center px-4 py-3 bg-slate-700/50 rounded-lg">
      <div className="text-2xl mb-1">üß†</div>
      <div className="text-sm text-slate-300">Trained Model</div>
      <div className="text-xs text-slate-500">float32</div>
    </div>
    <div className="text-slate-500 text-2xl">‚Üí</div>
    <div className="text-center px-4 py-3 bg-cyan-900/30 rounded-lg border border-cyan-700/30">
      <div className="text-2xl mb-1">‚öôÔ∏è</div>
      <div className="text-sm text-cyan-300">PTQ</div>
      <div className="text-xs text-slate-500">Convert weights</div>
    </div>
    <div className="text-slate-500 text-2xl">‚Üí</div>
    <div className="text-center px-4 py-3 bg-emerald-900/30 rounded-lg border border-emerald-700/30">
      <div className="text-2xl mb-1">üöÄ</div>
      <div className="text-sm text-emerald-300">Quantized Model</div>
      <div className="text-xs text-slate-500">int8/int4</div>
    </div>
  </div>
</div>

### Static vs Dynamic

<div className="grid md:grid-cols-2 gap-4 my-6">
  <div className="bg-slate-800/50 rounded-lg p-5 border border-slate-700">
    <div className="font-bold text-slate-200 mb-2">Static PTQ</div>
    <div className="text-sm text-slate-400 mb-3">Fixed scales from calibration data</div>
    <div className="text-xs space-y-1 text-slate-500">
      <div>‚Ä¢ Run sample data to find ranges</div>
      <div>‚Ä¢ Scales fixed at inference time</div>
      <div>‚Ä¢ Faster inference</div>
    </div>
  </div>
  <div className="bg-slate-800/50 rounded-lg p-5 border border-slate-700">
    <div className="font-bold text-slate-200 mb-2">Dynamic PTQ</div>
    <div className="text-sm text-slate-400 mb-3">Scales computed per input</div>
    <div className="text-xs space-y-1 text-slate-500">
      <div>‚Ä¢ No calibration needed</div>
      <div>‚Ä¢ Adapts to each input</div>
      <div>‚Ä¢ Slightly slower</div>
    </div>
  </div>
</div>

<Implementation>

```python
import torch
import torch.nn as nn

model = YourModel()

# Dynamic PTQ - just one line!
model_quantized = torch.quantization.quantize_dynamic(
    model,
    {nn.Linear},  # Which layers to quantize
    dtype=torch.qint8
)
```

</Implementation>

## Quantization-Aware Training (QAT)

QAT inserts "fake quantization" during training. The model learns to be robust to quantization noise.

<div className="my-8 p-6 bg-slate-800/30 rounded-xl border border-slate-700">
  <div className="text-center text-sm text-slate-400 mb-4">The Straight-Through Estimator Trick</div>

  <div className="grid md:grid-cols-2 gap-6">
    <div className="text-center">
      <div className="text-cyan-400 font-semibold mb-2">Forward Pass</div>
      <div className="bg-slate-700/50 rounded-lg p-3 font-mono text-sm">
        <div className="text-slate-400">y = <span className="text-cyan-400">quantize</span>(x)</div>
      </div>
      <div className="text-xs text-slate-500 mt-2">Simulate quantization error</div>
    </div>
    <div className="text-center">
      <div className="text-violet-400 font-semibold mb-2">Backward Pass</div>
      <div className="bg-slate-700/50 rounded-lg p-3 font-mono text-sm">
        <div className="text-slate-400">‚àÇL/‚àÇx = ‚àÇL/‚àÇy</div>
      </div>
      <div className="text-xs text-slate-500 mt-2">Gradient flows through unchanged</div>
    </div>
  </div>

  <div className="mt-4 text-center text-sm text-slate-300">
    Forward: apply quantization. Backward: pretend it didn't happen.
  </div>
</div>

<Mathematical>

Forward: $y = Q(x)$ where $Q$ is quantize-then-dequantize

Backward: $\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y}$ (straight-through)

This is mathematically "wrong" but works remarkably well in practice.

</Mathematical>

<Tip title="When to Use QAT">

- PTQ gives unacceptable accuracy loss
- You need aggressive quantization (`int4`, `int2`)
- You have compute budget for training/fine-tuning

</Tip>

## Modern LLM Methods

For large language models, specialized methods have emerged:

<div className="grid md:grid-cols-2 gap-4 my-8">
  <div className="bg-gradient-to-br from-amber-900/30 to-amber-800/10 rounded-xl p-6 border border-amber-700/50">
    <div className="text-amber-400 font-bold text-xl mb-3">GPTQ</div>
    <div className="text-slate-300 text-sm mb-4">Optimal Brain Quantization for LLMs</div>
    <div className="space-y-3 text-sm">
      <div className="flex items-start gap-2">
        <span className="text-amber-400 mt-1">‚Ä¢</span>
        <div>
          <span className="text-slate-200">Layer-wise optimization</span>
          <div className="text-slate-500 text-xs">Quantizes one layer at a time</div>
        </div>
      </div>
      <div className="flex items-start gap-2">
        <span className="text-amber-400 mt-1">‚Ä¢</span>
        <div>
          <span className="text-slate-200">Error compensation</span>
          <div className="text-slate-500 text-xs">Adjusts remaining weights to cancel error</div>
        </div>
      </div>
      <div className="flex items-start gap-2">
        <span className="text-amber-400 mt-1">‚Ä¢</span>
        <div>
          <span className="text-slate-200">Calibration data</span>
          <div className="text-slate-500 text-xs">Uses ~128 samples</div>
        </div>
      </div>
    </div>
  </div>
  <div className="bg-gradient-to-br from-emerald-900/30 to-emerald-800/10 rounded-xl p-6 border border-emerald-700/50">
    <div className="text-emerald-400 font-bold text-xl mb-3">AWQ</div>
    <div className="text-slate-300 text-sm mb-4">Activation-Aware Weight Quantization</div>
    <div className="space-y-3 text-sm">
      <div className="flex items-start gap-2">
        <span className="text-emerald-400 mt-1">‚Ä¢</span>
        <div>
          <span className="text-slate-200">Protects salient weights</span>
          <div className="text-slate-500 text-xs">Important weights get more precision</div>
        </div>
      </div>
      <div className="flex items-start gap-2">
        <span className="text-emerald-400 mt-1">‚Ä¢</span>
        <div>
          <span className="text-slate-200">Channel scaling</span>
          <div className="text-slate-500 text-xs">Scales up important channels before quantizing</div>
        </div>
      </div>
      <div className="flex items-start gap-2">
        <span className="text-emerald-400 mt-1">‚Ä¢</span>
        <div>
          <span className="text-slate-200">Activation-based</span>
          <div className="text-slate-500 text-xs">Importance judged by activation magnitude</div>
        </div>
      </div>
    </div>
  </div>
</div>

### How GPTQ Works

<Intuition>

GPTQ's insight: **weight errors can cancel out**. If one weight is rounded up, another can be rounded down to compensate.

Instead of independently rounding each weight, GPTQ:
1. Quantizes weights one at a time
2. After each quantization, adjusts remaining weights to minimize output error
3. Repeats until all weights are quantized

The result: much better accuracy than naive rounding.

</Intuition>

<div className="my-8 p-6 bg-slate-800/30 rounded-xl border border-slate-700">
  <div className="text-center text-sm text-slate-400 mb-4">GPTQ Error Compensation</div>

  <div className="flex items-center justify-center gap-3 flex-wrap">
    <div className="text-center">
      <div className="w-12 h-12 rounded-lg bg-amber-500/30 border border-amber-500/50 flex items-center justify-center font-mono text-sm">w‚ÇÅ</div>
      <div className="text-xs text-slate-500 mt-1">Quantize</div>
    </div>
    <div className="text-slate-500">‚Üí</div>
    <div className="text-center">
      <div className="flex gap-1">
        <div className="w-12 h-12 rounded-lg bg-slate-700/50 border border-slate-600 flex items-center justify-center font-mono text-sm">w‚ÇÇ</div>
        <div className="w-12 h-12 rounded-lg bg-slate-700/50 border border-slate-600 flex items-center justify-center font-mono text-sm">w‚ÇÉ</div>
        <div className="w-12 h-12 rounded-lg bg-slate-700/50 border border-slate-600 flex items-center justify-center font-mono text-sm">...</div>
      </div>
      <div className="text-xs text-slate-500 mt-1">Adjust to compensate</div>
    </div>
    <div className="text-slate-500">‚Üí</div>
    <div className="text-center">
      <div className="w-12 h-12 rounded-lg bg-amber-500/30 border border-amber-500/50 flex items-center justify-center font-mono text-sm">w‚ÇÇ</div>
      <div className="text-xs text-slate-500 mt-1">Next weight</div>
    </div>
  </div>
</div>

<Mathematical>

GPTQ minimizes layer-wise error:

$$\min_{Q} \|WX - QX\|_F^2$$

Using the Hessian $H = X^TX$, it updates remaining weights after quantizing $w_q$:

$$\delta w = -\frac{w_q - w}{H_{qq}^{-1}} H_q$$

</Mathematical>

### How AWQ Works

<Intuition>

AWQ's insight: **not all weights matter equally**. Weights that consistently produce large activations are "salient"‚Äîquantizing them hurts more.

AWQ protects salient weights by:
1. Identifying which input channels cause large activations
2. Scaling up weights for those channels (more quantization levels)
3. Scaling down the corresponding activations to compensate

</Intuition>

<Implementation>

```python
# AWQ concept (simplified)
def compute_saliency(weight, activations):
    """Which channels matter most?"""
    act_scale = activations.abs().mean(dim=0)
    weight_scale = weight.abs().mean(dim=0)
    return act_scale * weight_scale

saliency = compute_saliency(layer.weight, sample_activations)

# Scale up important channels before quantizing
scales = (saliency / saliency.max()).sqrt()
scaled_weight = layer.weight * scales

# Now quantize - important channels have more precision
quantized = quantize(scaled_weight)
```

</Implementation>

## Decision Flowchart

<div className="my-8 p-6 bg-slate-800/30 rounded-xl border border-slate-700">
  <div className="text-center text-lg font-semibold text-slate-200 mb-6">Which Method Should You Use?</div>

  <div className="space-y-4 max-w-md mx-auto">
    <div className="flex items-center gap-3 p-3 bg-slate-700/30 rounded-lg">
      <span className="text-2xl">üéØ</span>
      <div>
        <div className="text-slate-200 font-medium">Need `int8`?</div>
        <div className="text-slate-400 text-sm">‚Üí Dynamic PTQ (one line of code)</div>
      </div>
    </div>

    <div className="flex items-center gap-3 p-3 bg-slate-700/30 rounded-lg">
      <span className="text-2xl">ü§ñ</span>
      <div>
        <div className="text-slate-200 font-medium">Quantizing an LLM to `int4`?</div>
        <div className="text-slate-400 text-sm">‚Üí GPTQ or AWQ (most common)</div>
      </div>
    </div>

    <div className="flex items-center gap-3 p-3 bg-slate-700/30 rounded-lg">
      <span className="text-2xl">üìâ</span>
      <div>
        <div className="text-slate-200 font-medium">PTQ accuracy unacceptable?</div>
        <div className="text-slate-400 text-sm">‚Üí Try AWQ, then QAT if needed</div>
      </div>
    </div>

    <div className="flex items-center gap-3 p-3 bg-slate-700/30 rounded-lg">
      <span className="text-2xl">‚ö°</span>
      <div>
        <div className="text-slate-200 font-medium">Need extreme compression (int2)?</div>
        <div className="text-slate-400 text-sm">‚Üí QAT is your only option</div>
      </div>
    </div>
  </div>
</div>

## Quick Comparison

<div className="my-6 overflow-x-auto">
  <table className="w-full text-sm border-collapse">
    <thead>
      <tr className="bg-slate-800/50">
        <th className="p-3 text-left font-bold text-slate-200 rounded-tl-lg">Method</th>
        <th className="p-3 text-left font-bold text-slate-200">Speed</th>
        <th className="p-3 text-left font-bold text-slate-200">Bits</th>
        <th className="p-3 text-left font-bold text-slate-200">Accuracy</th>
        <th className="p-3 text-left font-bold text-slate-200 rounded-tr-lg">Best For</th>
      </tr>
    </thead>
    <tbody className="bg-slate-900/30">
      <tr className="border-t border-slate-700/50">
        <td className="p-3 text-slate-300">Dynamic PTQ</td>
        <td className="p-3 text-emerald-400">Fast</td>
        <td className="p-3">8</td>
        <td className="p-3">Good</td>
        <td className="p-3 text-slate-400">Quick start</td>
      </tr>
      <tr className="border-t border-slate-700/50">
        <td className="p-3 text-slate-300">Static PTQ</td>
        <td className="p-3 text-emerald-400">Fast</td>
        <td className="p-3">8</td>
        <td className="p-3">Good</td>
        <td className="p-3 text-slate-400">Production</td>
      </tr>
      <tr className="border-t border-slate-700/50">
        <td className="p-3 text-amber-400">GPTQ</td>
        <td className="p-3 text-amber-400">Medium</td>
        <td className="p-3">4</td>
        <td className="p-3">Great</td>
        <td className="p-3 text-slate-400">LLMs</td>
      </tr>
      <tr className="border-t border-slate-700/50">
        <td className="p-3 text-emerald-400">AWQ</td>
        <td className="p-3 text-amber-400">Medium</td>
        <td className="p-3">4</td>
        <td className="p-3">Great</td>
        <td className="p-3 text-slate-400">LLMs</td>
      </tr>
      <tr className="border-t border-slate-700/50">
        <td className="p-3 text-violet-400 rounded-bl-lg">QAT</td>
        <td className="p-3 text-red-400">Slow</td>
        <td className="p-3">2-8</td>
        <td className="p-3">Best</td>
        <td className="p-3 text-slate-400 rounded-br-lg">Extreme</td>
      </tr>
    </tbody>
  </table>
</div>

<Warning title="Always Measure">

Quantization can fail silently. Your model might look fine on average but fail on specific inputs.

**Always benchmark after quantizing:**
- Run standard evaluation metrics
- Test edge cases
- Check specific failure modes

</Warning>

## Next Up

<Note>

Theory complete! Let's put it into practice. The next section walks through quantizing a real LLM.

Continue to [Quantization in Practice](/chapters/quantization/quantization-in-practice).

</Note>
