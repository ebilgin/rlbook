---
title: "Quantization"
slug: "quantization"
section: "ML Concepts"
description: "Reducing model size and speeding up inference by using lower-precision numbers"
status: "draft"
lastReviewed: null
prerequisites:
  - slug: "function-approximation"
    title: "Function Approximation"
---

import { Intuition, Mathematical, Implementation } from '@/components/ui/ContentLayers';
import { ChapterObjectives, KeyTakeaways, NextChapter } from '@/components/ui/ChapterNav';
import { Note, Warning, Tip, Example, Definition } from '@/components/ui/Callouts';
import QuantizationDemo from '@/components/interactive/QuantizationDemo';
import MemoryCalculator from '@/components/interactive/MemoryCalculator';

# Quantization

<ChapterObjectives>
- Understand why quantization is essential for deploying large models
- See how reducing precision affects neural network weights
- Learn the memory-accuracy tradeoff through interactive exploration
</ChapterObjectives>

A 7-billion parameter model needs **28 GB** just to store its weights:

<div className="my-4 p-4 bg-slate-800/50 rounded-lg border border-slate-700 font-mono text-sm">
  <div className="flex flex-wrap items-center gap-2">
    <span className="text-slate-300">7B params</span>
    <span className="text-slate-500">Ã—</span>
    <span className="text-cyan-400">32 bits</span>
    <span className="text-slate-500">Ã·</span>
    <span className="text-slate-300">8 bits/byte</span>
    <span className="text-slate-500">=</span>
    <span className="text-amber-400 font-bold">28 GB</span>
  </div>
  <div className="text-xs text-slate-500 mt-2">float32 = 32 bits = 4 bytes per number</div>
</div>

Most GPUs have 8-24 GB. Quantization shrinks models by storing each parameter in fewer bits.

## Try It First

What if we used fewer bits? Drag the slider to see how precision affects the values:

<QuantizationDemo client:load />

<Intuition>

**What you just saw:** Neural network weights are numbers (like 0.0342 or -0.1567). When we reduce precision from 32 bits to 8 bits, these values "snap" to a grid of allowed values.

The magic? With 256 levels (8-bit), the error is tiny. Most models barely notice the difference.

</Intuition>

## The Core Tradeoff

<div className="grid md:grid-cols-4 gap-3 my-8">
  <div className="bg-slate-800/50 rounded-lg p-4 text-center border border-slate-700">
    <div className="text-2xl font-bold text-slate-300">float32</div>
    <div className="text-slate-500 text-sm">32 bits</div>
    <div className="text-slate-400 text-xs mt-2">Baseline</div>
  </div>
  <div className="bg-gradient-to-br from-blue-900/30 to-blue-800/10 rounded-lg p-4 text-center border border-blue-700/50">
    <div className="text-2xl font-bold text-blue-400">float16</div>
    <div className="text-slate-400 text-sm">16 bits</div>
    <div className="text-blue-300 text-xs mt-2">2Ã— smaller</div>
  </div>
  <div className="bg-gradient-to-br from-emerald-900/30 to-emerald-800/10 rounded-lg p-4 text-center border border-emerald-700/50">
    <div className="text-2xl font-bold text-emerald-400">int8</div>
    <div className="text-slate-400 text-sm">8 bits</div>
    <div className="text-emerald-300 text-xs mt-2">4Ã— smaller</div>
  </div>
  <div className="bg-gradient-to-br from-amber-900/30 to-amber-800/10 rounded-lg p-4 text-center border border-amber-700/50">
    <div className="text-2xl font-bold text-amber-400">int4</div>
    <div className="text-slate-400 text-sm">4 bits</div>
    <div className="text-amber-300 text-xs mt-2">8Ã— smaller</div>
  </div>
</div>

<Definition title="Quantization">
Mapping continuous (or high-precision) values to a smaller set of discrete values. In neural networks: converting 32-bit weights to 16-bit, 8-bit, or 4-bit.
</Definition>

## Why This Matters for RL

<div className="grid md:grid-cols-2 gap-4 my-6">
  <div className="flex items-start gap-3 bg-slate-800/30 rounded-lg p-4 border border-slate-700">
    <span className="text-2xl">ðŸ¤–</span>
    <div>
      <div className="font-semibold text-slate-200">Real-time Control</div>
      <div className="text-sm text-slate-400">Robots need millisecond decisions</div>
    </div>
  </div>
  <div className="flex items-start gap-3 bg-slate-800/30 rounded-lg p-4 border border-slate-700">
    <span className="text-2xl">ðŸŽ®</span>
    <div>
      <div className="font-semibold text-slate-200">Game AI</div>
      <div className="text-sm text-slate-400">60 FPS requires fast inference</div>
    </div>
  </div>
  <div className="flex items-start gap-3 bg-slate-800/30 rounded-lg p-4 border border-slate-700">
    <span className="text-2xl">ðŸ’¬</span>
    <div>
      <div className="font-semibold text-slate-200">RLHF Models</div>
      <div className="text-sm text-slate-400">Quantized LLMs train and serve faster</div>
    </div>
  </div>
  <div className="flex items-start gap-3 bg-slate-800/30 rounded-lg p-4 border border-slate-700">
    <span className="text-2xl">ðŸ“±</span>
    <div>
      <div className="font-semibold text-slate-200">Edge Deployment</div>
      <div className="text-sm text-slate-400">Drones and cars can't carry data centers</div>
    </div>
  </div>
</div>

## Chapter Roadmap

<div className="grid md:grid-cols-2 gap-4 my-6">
  <a href="/chapters/quantization/why-quantization" className="block p-4 bg-gradient-to-br from-cyan-900/30 to-cyan-800/10 rounded-lg border border-cyan-700/50 hover:border-cyan-500/50 transition-colors group">
    <div className="flex items-center gap-2 mb-1">
      <span className="text-cyan-400 font-mono text-sm">01</span>
      <h3 className="font-semibold text-cyan-400 group-hover:text-cyan-300">Why Quantization Matters</h3>
    </div>
    <p className="text-slate-400 text-sm">Memory problem, speed gains, and when it works</p>
  </a>
  <a href="/chapters/quantization/number-representations" className="block p-4 bg-gradient-to-br from-violet-900/30 to-violet-800/10 rounded-lg border border-violet-700/50 hover:border-violet-500/50 transition-colors group">
    <div className="flex items-center gap-2 mb-1">
      <span className="text-violet-400 font-mono text-sm">02</span>
      <h3 className="font-semibold text-violet-400 group-hover:text-violet-300">Number Representations</h3>
    </div>
    <p className="text-slate-400 text-sm">float32, float16, int8: how computers store numbers</p>
  </a>
  <a href="/chapters/quantization/quantization-methods" className="block p-4 bg-gradient-to-br from-amber-900/30 to-amber-800/10 rounded-lg border border-amber-700/50 hover:border-amber-500/50 transition-colors group">
    <div className="flex items-center gap-2 mb-1">
      <span className="text-amber-400 font-mono text-sm">03</span>
      <h3 className="font-semibold text-amber-400 group-hover:text-amber-300">Quantization Methods</h3>
    </div>
    <p className="text-slate-400 text-sm">PTQ, QAT, GPTQ, and AWQ explained</p>
  </a>
  <a href="/chapters/quantization/quantization-in-practice" className="block p-4 bg-gradient-to-br from-emerald-900/30 to-emerald-800/10 rounded-lg border border-emerald-700/50 hover:border-emerald-500/50 transition-colors group">
    <div className="flex items-center gap-2 mb-1">
      <span className="text-emerald-400 font-mono text-sm">04</span>
      <h3 className="font-semibold text-emerald-400 group-hover:text-emerald-300">Hands-On Practice</h3>
    </div>
    <p className="text-slate-400 text-sm">Quantize a real LLM in our Jupyter notebook</p>
  </a>
</div>

<Note title="Start Here">

New to quantization? Begin with [Why Quantization Matters](/chapters/quantization/why-quantization) to understand the motivation.

Want to jump straight to code? Head to [Quantization in Practice](/chapters/quantization/quantization-in-practice).

</Note>

---

<KeyTakeaways>
- Quantization reduces model memory by using fewer bits per parameter
- 8-bit quantization typically loses less than 1% accuracy
- Modern methods (GPTQ, AWQ) make 4-bit quantization practical for LLMs
- Essential for deploying RL policies in real-time
</KeyTakeaways>
