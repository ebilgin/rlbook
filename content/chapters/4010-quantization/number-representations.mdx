---
title: "Number Representations"
slug: "number-representations"
section: "ML Concepts"
description: "From float32 to int8: how computers store numbers"
---

import { Intuition, Mathematical, Implementation } from '@/components/ui/ContentLayers';
import { Note, Warning, Tip, Example, Definition } from '@/components/ui/Callouts';
import BitLayoutVisualizer from '@/components/interactive/BitLayoutVisualizer';

# Number Representations

To understand quantization, you need to understand how computers store numbers. Let's explore the formats you'll encounter.

## Explore Float Formats

**Try entering different numbers** to see how they're stored in each format:

<BitLayoutVisualizer client:load />

## Floating Point Basics

<Intuition>

Floating point is like scientific notation. Instead of writing 0.000000123, you write 1.23 × 10⁻⁷.

A float32 has three parts:
- **Sign** (1 bit): positive or negative
- **Exponent** (8 bits): the power of 2
- **Mantissa** (23 bits): the precision digits

</Intuition>

<div className="my-6 p-4 bg-slate-800/50 rounded-lg border border-slate-700">
  <div className="text-center text-sm text-slate-400 mb-3">float32 = 32 bits total</div>
  <div className="flex justify-center gap-1 font-mono text-xs sm:text-sm">
    <div className="bg-red-600/40 px-2 sm:px-3 py-2 rounded border border-red-500/30">S</div>
    <div className="bg-emerald-600/40 px-1.5 sm:px-2 py-2 rounded border border-emerald-500/30">E E E E E E E E</div>
    <div className="bg-blue-600/40 px-1 py-2 rounded border border-blue-500/30 text-xs">M M M M M M M M M M M M M M M M M M M M M M M</div>
  </div>
  <div className="flex justify-center gap-4 mt-3 text-xs">
    <span className="flex items-center gap-1"><span className="w-2 h-2 rounded bg-red-500/70"></span> Sign (1)</span>
    <span className="flex items-center gap-1"><span className="w-2 h-2 rounded bg-emerald-500/70"></span> Exponent (8)</span>
    <span className="flex items-center gap-1"><span className="w-2 h-2 rounded bg-blue-500/70"></span> Mantissa (23)</span>
  </div>
</div>

<Mathematical>

A floating point number represents:

$$(-1)^{sign} \times (1 + mantissa) \times 2^{exponent - bias}$$

For float32, the bias is 127. So exponent bits of 10000001 (129) means 2^(129-127) = 2² = 4.

</Mathematical>

## Float16 vs BFloat16

The two 16-bit formats make different tradeoffs:

<div className="grid md:grid-cols-2 gap-4 my-8">
  <div className="bg-gradient-to-br from-violet-900/30 to-violet-800/10 rounded-xl p-5 border border-violet-700/50">
    <div className="text-violet-400 font-bold text-lg mb-3">float16</div>
    <div className="flex gap-1 font-mono text-xs mb-3">
      <div className="bg-red-600/40 px-2 py-1 rounded">S</div>
      <div className="bg-emerald-600/40 px-1 py-1 rounded">E E E E E</div>
      <div className="bg-blue-600/40 px-1 py-1 rounded">M M M M M M M M M M</div>
    </div>
    <div className="text-slate-400 text-sm space-y-1">
      <div>• <strong className="text-slate-200">5 exponent bits</strong> → smaller range</div>
      <div>• <strong className="text-slate-200">10 mantissa bits</strong> → more precision</div>
      <div>• Max: ~65,504</div>
      <div>• Best for: Inference</div>
    </div>
  </div>
  <div className="bg-gradient-to-br from-amber-900/30 to-amber-800/10 rounded-xl p-5 border border-amber-700/50">
    <div className="text-amber-400 font-bold text-lg mb-3">bfloat16</div>
    <div className="flex gap-1 font-mono text-xs mb-3">
      <div className="bg-red-600/40 px-2 py-1 rounded">S</div>
      <div className="bg-emerald-600/40 px-1 py-1 rounded">E E E E E E E E</div>
      <div className="bg-blue-600/40 px-1 py-1 rounded">M M M M M M M</div>
    </div>
    <div className="text-slate-400 text-sm space-y-1">
      <div>• <strong className="text-slate-200">8 exponent bits</strong> → same range as float32</div>
      <div>• <strong className="text-slate-200">7 mantissa bits</strong> → less precision</div>
      <div>• Max: ~3.4×10³⁸</div>
      <div>• Best for: Training on TPUs</div>
    </div>
  </div>
</div>

<Example title="Why This Matters">

```python
x = 100000.0

# float16: OVERFLOW! Returns inf
x.half()  # → inf

# bfloat16: Works fine (with precision loss)
x.bfloat16()  # → 100352.0
```

Training involves large intermediate values. bfloat16's range prevents overflow.

</Example>

## Integer Quantization

<Intuition>

Integer quantization is simpler: we pick a **scale factor** and divide all values by it. The result is an integer.

For int8: we map the range [-max, +max] to [-127, +127].

</Intuition>

<div className="my-8 p-6 bg-slate-800/30 rounded-xl border border-slate-700">
  <div className="text-center text-sm text-slate-400 mb-4">Symmetric Quantization</div>

  <div className="flex items-center justify-center gap-3 mb-4">
    <div className="text-center">
      <div className="text-lg font-mono text-cyan-400">0.247</div>
      <div className="text-xs text-slate-500">float32</div>
    </div>
    <div className="text-slate-500">→</div>
    <div className="text-center bg-slate-700/50 px-3 py-2 rounded">
      <div className="text-xs text-slate-400 mb-1">÷ scale</div>
      <div className="font-mono text-slate-300">÷ 0.00195</div>
    </div>
    <div className="text-slate-500">→</div>
    <div className="text-center bg-slate-700/50 px-3 py-2 rounded">
      <div className="text-xs text-slate-400 mb-1">round</div>
      <div className="font-mono text-slate-300">127</div>
    </div>
    <div className="text-slate-500">→</div>
    <div className="text-center">
      <div className="text-lg font-mono text-emerald-400">127</div>
      <div className="text-xs text-slate-500">int8</div>
    </div>
  </div>

  <div className="text-center text-sm text-slate-400">
    scale = max_value / 127
  </div>
</div>

<Mathematical>

For b-bit symmetric quantization:

$$q = \text{round}\left(\frac{x}{s}\right), \quad s = \frac{\max(|x|)}{2^{b-1}-1}$$

To recover the approximate value:

$$\hat{x} = q \times s$$

</Mathematical>

<Implementation>

```python
import numpy as np

def quantize_symmetric(x, bits=8):
    qmax = 2**(bits-1) - 1  # 127 for int8
    scale = np.max(np.abs(x)) / qmax
    q = np.round(x / scale).clip(-qmax, qmax).astype(np.int8)
    return q, scale

# Example
weights = np.array([0.247, -0.103, 0.089, -0.156])
q, scale = quantize_symmetric(weights)
print(f"Quantized: {q}")  # [127, -53, 46, -80]
print(f"Scale: {scale:.5f}")  # 0.00194
```

</Implementation>

## Symmetric vs Asymmetric

<div className="grid md:grid-cols-2 gap-4 my-8">
  <div className="bg-slate-800/50 rounded-xl p-5 border border-slate-700">
    <div className="text-slate-200 font-bold mb-3">Symmetric</div>
    <div className="text-4xl mb-3 text-center">−127 ↔ 0 ↔ +127</div>
    <div className="text-slate-400 text-sm">
      <div>• Zero maps to zero exactly</div>
      <div>• Good for weights (centered around 0)</div>
      <div>• Simpler computation</div>
    </div>
  </div>
  <div className="bg-slate-800/50 rounded-xl p-5 border border-slate-700">
    <div className="text-slate-200 font-bold mb-3">Asymmetric</div>
    <div className="text-4xl mb-3 text-center">0 ↔ zero_pt ↔ 255</div>
    <div className="text-slate-400 text-sm">
      <div>• Adds a "zero point" offset</div>
      <div>• Good for ReLU activations (all positive)</div>
      <div>• Uses full range efficiently</div>
    </div>
  </div>
</div>

<Tip title="Rule of Thumb">

- **Weights**: Usually symmetric (centered near zero)
- **Activations**: Often asymmetric (ReLU outputs are positive)

</Tip>

## Per-Tensor vs Per-Channel

<Intuition>

Should you use one scale for the whole layer, or one scale per channel?

Different channels can have very different weight magnitudes. Per-channel quantization adapts to each.

</Intuition>

<div className="my-6 p-4 bg-slate-800/50 rounded-lg border border-slate-700">
  <div className="text-sm text-slate-400 mb-3">Example: 4 channels with different scales</div>
  <div className="space-y-2">
    <div className="flex items-center gap-3">
      <span className="text-xs text-slate-500 w-16">Channel 0</span>
      <div className="flex-1 h-3 bg-cyan-500/20 rounded relative">
        <div className="absolute left-0 top-0 h-full w-[10%] bg-cyan-400 rounded"></div>
      </div>
      <span className="text-xs text-slate-400">small weights</span>
    </div>
    <div className="flex items-center gap-3">
      <span className="text-xs text-slate-500 w-16">Channel 1</span>
      <div className="flex-1 h-3 bg-cyan-500/20 rounded relative">
        <div className="absolute left-0 top-0 h-full w-[50%] bg-cyan-400 rounded"></div>
      </div>
      <span className="text-xs text-slate-400">medium weights</span>
    </div>
    <div className="flex items-center gap-3">
      <span className="text-xs text-slate-500 w-16">Channel 2</span>
      <div className="flex-1 h-3 bg-cyan-500/20 rounded relative">
        <div className="absolute left-0 top-0 h-full w-[90%] bg-cyan-400 rounded"></div>
      </div>
      <span className="text-xs text-slate-400">large weights</span>
    </div>
    <div className="flex items-center gap-3">
      <span className="text-xs text-slate-500 w-16">Channel 3</span>
      <div className="flex-1 h-3 bg-cyan-500/20 rounded relative">
        <div className="absolute left-0 top-0 h-full w-[5%] bg-cyan-400 rounded"></div>
      </div>
      <span className="text-xs text-slate-400">tiny weights</span>
    </div>
  </div>
  <div className="mt-4 text-sm text-slate-300 text-center">
    Per-channel: each row gets its own scale → much better precision
  </div>
</div>

<Implementation>

```python
def per_channel_quantize(weights, bits=8):
    """Per-channel quantization: one scale per output channel."""
    qmax = 2**(bits-1) - 1

    # One scale per row (output channel)
    scales = np.max(np.abs(weights), axis=1) / qmax

    # Quantize each channel with its scale
    q = np.round(weights / scales[:, None]).astype(np.int8)

    return q, scales

# Comparison
weights = np.random.randn(4, 64) * np.array([[0.1], [1.0], [10.0], [0.01]])

mse_per_tensor = compute_mse(per_tensor_quantize(weights))
mse_per_channel = compute_mse(per_channel_quantize(weights))

print(f"Per-tensor MSE: {mse_per_tensor:.6f}")
print(f"Per-channel MSE: {mse_per_channel:.6f}")
# Per-channel is typically 10-100x better!
```

</Implementation>

## Quick Reference

<div className="my-6">
  <div className="grid grid-cols-2 md:grid-cols-3 gap-3">
    <div className="bg-slate-800/50 rounded-lg p-4 border border-slate-700">
      <div className="font-bold text-slate-200 mb-2">float32</div>
      <div className="text-xs text-slate-400 space-y-1">
        <div>Range: ±3.4×10³⁸</div>
        <div>Precision: ~7 digits</div>
        <div>Use: Training baseline</div>
      </div>
    </div>
    <div className="bg-gradient-to-br from-blue-900/20 to-blue-800/5 rounded-lg p-4 border border-blue-700/40">
      <div className="font-bold text-blue-400 mb-2">float16</div>
      <div className="text-xs text-slate-400 space-y-1">
        <div>Range: ±65,504</div>
        <div>Precision: ~3 digits</div>
        <div>Use: Inference</div>
      </div>
    </div>
    <div className="bg-gradient-to-br from-amber-900/20 to-amber-800/5 rounded-lg p-4 border border-amber-700/40">
      <div className="font-bold text-amber-400 mb-2">bfloat16</div>
      <div className="text-xs text-slate-400 space-y-1">
        <div>Range: ±3.4×10³⁸</div>
        <div>Precision: ~2 digits</div>
        <div>Use: Training on TPUs</div>
      </div>
    </div>
    <div className="bg-gradient-to-br from-emerald-900/20 to-emerald-800/5 rounded-lg p-4 border border-emerald-700/40">
      <div className="font-bold text-emerald-400 mb-2">int8</div>
      <div className="text-xs text-slate-400 space-y-1">
        <div>Range: -128 to 127</div>
        <div>Levels: 256</div>
        <div>Use: Standard inference</div>
      </div>
    </div>
    <div className="bg-gradient-to-br from-violet-900/20 to-violet-800/5 rounded-lg p-4 border border-violet-700/40">
      <div className="font-bold text-violet-400 mb-2">int4</div>
      <div className="text-xs text-slate-400 space-y-1">
        <div>Range: -8 to 7</div>
        <div>Levels: 16</div>
        <div>Use: LLM compression</div>
      </div>
    </div>
  </div>
</div>

## Next Up

<Note>

Now that you understand number formats, let's see the **methods** for quantizing models: PTQ, QAT, GPTQ, and AWQ.

Continue to [Quantization Methods](/chapters/quantization/quantization-methods).

</Note>
