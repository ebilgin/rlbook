---
title: "Quantization in Practice"
slug: "quantization-in-practice"
section: "ML Concepts"
description: "Tools, techniques, and hands-on examples"
---

import { Intuition, Mathematical, Implementation } from '@/components/ui/ContentLayers';
import { Note, Warning, Tip, Example, Definition } from '@/components/ui/Callouts';

# Quantization in Practice

Time to get hands-on. We'll quantize a real LLM and measure the tradeoffs.

## The Practical Workflow

<div className="my-8 p-6 bg-slate-800/30 rounded-xl border border-slate-700">
  <div className="flex items-center justify-center gap-3 flex-wrap text-sm">
    <div className="text-center px-4 py-3 bg-cyan-900/30 rounded-lg border border-cyan-700/30">
      <div className="text-2xl mb-1">1Ô∏è‚É£</div>
      <div className="text-cyan-300">Load Model</div>
    </div>
    <div className="text-slate-500 text-xl">‚Üí</div>
    <div className="text-center px-4 py-3 bg-violet-900/30 rounded-lg border border-violet-700/30">
      <div className="text-2xl mb-1">2Ô∏è‚É£</div>
      <div className="text-violet-300">Quantize</div>
    </div>
    <div className="text-slate-500 text-xl">‚Üí</div>
    <div className="text-center px-4 py-3 bg-amber-900/30 rounded-lg border border-amber-700/30">
      <div className="text-2xl mb-1">3Ô∏è‚É£</div>
      <div className="text-amber-300">Evaluate</div>
    </div>
    <div className="text-slate-500 text-xl">‚Üí</div>
    <div className="text-center px-4 py-3 bg-emerald-900/30 rounded-lg border border-emerald-700/30">
      <div className="text-2xl mb-1">4Ô∏è‚É£</div>
      <div className="text-emerald-300">Deploy</div>
    </div>
  </div>
</div>

## Quick Start: bitsandbytes

The fastest path to quantization. One flag, and your model loads in lower precision.

<div className="grid md:grid-cols-3 gap-4 my-6">
  <div className="bg-slate-800/50 rounded-lg p-4 border border-slate-700">
    <div className="font-mono text-blue-400 text-sm mb-2">float16</div>
    <div className="text-xs text-slate-400">torch_dtype=torch.float16</div>
    <div className="text-xs text-slate-500 mt-2">Baseline</div>
  </div>
  <div className="bg-gradient-to-br from-emerald-900/20 to-emerald-800/5 rounded-lg p-4 border border-emerald-700/40">
    <div className="font-mono text-emerald-400 text-sm mb-2">8-bit</div>
    <div className="text-xs text-slate-400">load_in_8bit=True</div>
    <div className="text-xs text-emerald-300 mt-2">2√ó smaller</div>
  </div>
  <div className="bg-gradient-to-br from-amber-900/20 to-amber-800/5 rounded-lg p-4 border border-amber-700/40">
    <div className="font-mono text-amber-400 text-sm mb-2">4-bit</div>
    <div className="text-xs text-slate-400">load_in_4bit=True</div>
    <div className="text-xs text-amber-300 mt-2">4√ó smaller</div>
  </div>
</div>

<Implementation>

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

MODEL = "Qwen/Qwen2.5-0.5B"

# Float16 baseline
model_fp16 = AutoModelForCausalLM.from_pretrained(
    MODEL, torch_dtype=torch.float16, device_map="auto"
)

# 8-bit - just add one flag
model_8bit = AutoModelForCausalLM.from_pretrained(
    MODEL, load_in_8bit=True, device_map="auto"
)

# 4-bit with NF4 (best quality)
config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)
model_4bit = AutoModelForCausalLM.from_pretrained(
    MODEL, quantization_config=config, device_map="auto"
)
```

</Implementation>

## GPTQ for Production

For best 4-bit quality, use GPTQ with calibration data.

<div className="my-8 p-6 bg-slate-800/30 rounded-xl border border-slate-700">
  <div className="text-center text-sm text-slate-400 mb-4">GPTQ Process</div>

  <div className="flex items-center justify-center gap-4 flex-wrap">
    <div className="text-center px-3 py-2 bg-slate-700/50 rounded-lg">
      <div className="text-slate-300 text-sm">Calibration Data</div>
      <div className="text-xs text-slate-500">~128 samples</div>
    </div>
    <div className="text-slate-500">+</div>
    <div className="text-center px-3 py-2 bg-slate-700/50 rounded-lg">
      <div className="text-slate-300 text-sm">Pre-trained Model</div>
      <div className="text-xs text-slate-500">float16</div>
    </div>
    <div className="text-slate-500 text-xl">‚Üí</div>
    <div className="text-center px-3 py-2 bg-amber-900/30 rounded-lg border border-amber-700/30">
      <div className="text-amber-300 text-sm">GPTQ</div>
      <div className="text-xs text-slate-400">Layer-by-layer</div>
    </div>
    <div className="text-slate-500 text-xl">‚Üí</div>
    <div className="text-center px-3 py-2 bg-emerald-900/30 rounded-lg border border-emerald-700/30">
      <div className="text-emerald-300 text-sm">4-bit Model</div>
      <div className="text-xs text-slate-400">Optimized</div>
    </div>
  </div>
</div>

<Implementation>

```python
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

# Prepare calibration data (use domain-representative text)
calibration_data = [
    tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
    for text in your_sample_texts[:128]
]

# Configure GPTQ
config = BaseQuantizeConfig(
    bits=4,
    group_size=128,
    desc_act=True,  # Helps with quality
)

# Quantize
model = AutoGPTQForCausalLM.from_pretrained(MODEL, config)
model.quantize(calibration_data)
model.save_quantized("./model-gptq-4bit")
```

</Implementation>

## Measuring Quality

<Warning title="Never Skip Evaluation">

Quantization can fail silently. A model might look fine on average but fail on specific inputs.

</Warning>

### Perplexity Test

<div className="my-6 p-4 bg-slate-800/50 rounded-lg border border-slate-700">
  <div className="text-sm text-slate-400 mb-3">Expected Results (Qwen-0.5B)</div>
  <div className="grid grid-cols-3 gap-4 text-center">
    <div>
      <div className="text-lg font-mono text-blue-400">~15.2</div>
      <div className="text-xs text-slate-500">float16</div>
    </div>
    <div>
      <div className="text-lg font-mono text-emerald-400">~15.4</div>
      <div className="text-xs text-slate-500">8-bit (+1%)</div>
    </div>
    <div>
      <div className="text-lg font-mono text-amber-400">~15.8</div>
      <div className="text-xs text-slate-500">4-bit (+4%)</div>
    </div>
  </div>
  <div className="text-xs text-slate-500 text-center mt-3">Lower is better</div>
</div>

<Implementation>

```python
from datasets import load_dataset

def evaluate_perplexity(model, tokenizer, max_samples=100):
    dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
    total_loss, total_tokens = 0, 0

    for sample in dataset.select(range(max_samples)):
        inputs = tokenizer(sample["text"], return_tensors="pt", truncation=True)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        with torch.no_grad():
            loss = model(**inputs, labels=inputs["input_ids"]).loss

        total_loss += loss.item() * inputs["input_ids"].shape[1]
        total_tokens += inputs["input_ids"].shape[1]

    return np.exp(total_loss / total_tokens)

# Compare
for name, model in [("FP16", model_fp16), ("8-bit", model_8bit), ("4-bit", model_4bit)]:
    ppl = evaluate_perplexity(model, tokenizer)
    print(f"{name}: {ppl:.2f}")
```

</Implementation>

## Speed Benchmarks

<div className="my-6 p-4 bg-slate-800/50 rounded-lg border border-slate-700">
  <div className="text-sm text-slate-400 mb-3">Typical Speedups</div>
  <div className="space-y-3">
    <div className="flex items-center gap-3">
      <span className="text-xs text-slate-500 w-12">float16</span>
      <div className="flex-1 h-4 bg-slate-700 rounded relative">
        <div className="absolute left-0 top-0 h-full w-[50%] bg-blue-400 rounded"></div>
      </div>
      <span className="text-xs text-slate-400">1.0√ó</span>
    </div>
    <div className="flex items-center gap-3">
      <span className="text-xs text-slate-500 w-12">8-bit</span>
      <div className="flex-1 h-4 bg-slate-700 rounded relative">
        <div className="absolute left-0 top-0 h-full w-[75%] bg-emerald-400 rounded"></div>
      </div>
      <span className="text-xs text-emerald-400">1.5√ó</span>
    </div>
    <div className="flex items-center gap-3">
      <span className="text-xs text-slate-500 w-12">4-bit</span>
      <div className="flex-1 h-4 bg-slate-700 rounded relative">
        <div className="absolute left-0 top-0 h-full w-[100%] bg-amber-400 rounded"></div>
      </div>
      <span className="text-xs text-amber-400">2.0√ó</span>
    </div>
  </div>
  <div className="text-xs text-slate-500 text-center mt-3">Results vary by hardware and model</div>
</div>

<Implementation>

```python
import time

def benchmark_speed(model, tokenizer, prompt, n_tokens=100, n_runs=5):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # Warmup
    with torch.no_grad():
        model.generate(**inputs, max_new_tokens=10)

    # Benchmark
    torch.cuda.synchronize()
    start = time.time()
    for _ in range(n_runs):
        with torch.no_grad():
            model.generate(**inputs, max_new_tokens=n_tokens, do_sample=False)
    torch.cuda.synchronize()

    total_time = time.time() - start
    tokens_per_sec = (n_tokens * n_runs) / total_time
    return tokens_per_sec
```

</Implementation>

## For RL Policies

<Intuition>

RL has special needs:
- **Policy network**: Speed matters most (many rollouts)
- **Value network**: Accuracy matters most (training signal)

Solution: Quantize the policy more aggressively than the critic.

</Intuition>

<div className="grid md:grid-cols-2 gap-4 my-6">
  <div className="bg-gradient-to-br from-cyan-900/30 to-cyan-800/10 rounded-lg p-4 border border-cyan-700/50">
    <div className="text-cyan-400 font-bold mb-2">Policy Network</div>
    <div className="text-sm text-slate-400">int8 quantization</div>
    <div className="text-xs text-slate-500 mt-2">Fast inference for rollouts</div>
  </div>
  <div className="bg-gradient-to-br from-violet-900/30 to-violet-800/10 rounded-lg p-4 border border-violet-700/50">
    <div className="text-violet-400 font-bold mb-2">Value Network</div>
    <div className="text-sm text-slate-400">float16 precision</div>
    <div className="text-xs text-slate-500 mt-2">Accurate value estimates</div>
  </div>
</div>

<Implementation>

```python
class QuantizedRLPolicy(nn.Module):
    def __init__(self, policy_net, value_net):
        super().__init__()
        # Quantize policy for speed
        self.policy = torch.quantization.quantize_dynamic(
            policy_net, {nn.Linear}, dtype=torch.qint8
        )
        # Keep value at higher precision
        self.value = value_net.half()

    def forward(self, state):
        action_logits = self.policy(state)
        value = self.value(state.half())
        return action_logits, value
```

</Implementation>

## Complete Notebook

Run the full workflow yourself:

<div className="flex flex-wrap gap-3 my-4">
  <a href="https://colab.research.google.com/github/ebilgin/rlbook/blob/main/public/notebooks/quantize-llm.ipynb" target="_blank" rel="noopener noreferrer" className="inline-flex items-center gap-2 px-4 py-3 bg-gradient-to-r from-amber-600 to-orange-600 text-white rounded-lg hover:from-amber-500 hover:to-orange-500 transition-all">
    <span className="text-xl">üöÄ</span>
    <div>
      <div className="font-semibold">Open in Colab</div>
      <div className="text-xs text-amber-100">Run with free GPU</div>
    </div>
  </a>
  <a href="https://github.com/ebilgin/rlbook/blob/main/public/notebooks/quantize-llm.ipynb" target="_blank" rel="noopener noreferrer" className="inline-flex items-center gap-2 px-4 py-3 bg-slate-700 text-white rounded-lg hover:bg-slate-600 transition-all">
    <span className="text-xl">üìì</span>
    <div>
      <div className="font-semibold">View on GitHub</div>
      <div className="text-xs text-slate-300">Browse source</div>
    </div>
  </a>
</div>

<div className="mt-4 text-sm text-slate-400">
The notebook includes:
</div>

<div className="grid md:grid-cols-2 gap-3 mt-3">
  <div className="flex items-center gap-2 text-sm text-slate-400">
    <span className="text-emerald-400">‚úì</span> Load Qwen-0.5B in fp16, 8-bit, 4-bit
  </div>
  <div className="flex items-center gap-2 text-sm text-slate-400">
    <span className="text-emerald-400">‚úì</span> Memory comparison
  </div>
  <div className="flex items-center gap-2 text-sm text-slate-400">
    <span className="text-emerald-400">‚úì</span> Perplexity evaluation
  </div>
  <div className="flex items-center gap-2 text-sm text-slate-400">
    <span className="text-emerald-400">‚úì</span> Speed benchmarks
  </div>
  <div className="flex items-center gap-2 text-sm text-slate-400">
    <span className="text-emerald-400">‚úì</span> GPTQ quantization
  </div>
  <div className="flex items-center gap-2 text-sm text-slate-400">
    <span className="text-emerald-400">‚úì</span> Runs on free Colab T4
  </div>
</div>

## Quick Reference

<Tip title="Best Practices">

1. **Start with `float16`** ‚Üí nearly lossless baseline
2. **Try `int8` first** ‚Üí usually good enough
3. **Use GPTQ/AWQ for `int4`** ‚Üí better than bitsandbytes
4. **Always measure** ‚Üí perplexity, task accuracy, speed
5. **Test edge cases** ‚Üí unusual inputs reveal problems

</Tip>

<div className="my-6 p-4 bg-slate-800/30 rounded-lg border border-slate-700">
  <div className="text-slate-200 font-semibold mb-3">Decision Tree</div>
  <div className="space-y-2 text-sm">
    <div className="flex items-start gap-2">
      <span className="text-slate-500">‚Ä¢</span>
      <div><span className="text-slate-300">Need quick results?</span> <span className="text-cyan-400">‚Üí bitsandbytes load_in_8bit</span></div>
    </div>
    <div className="flex items-start gap-2">
      <span className="text-slate-500">‚Ä¢</span>
      <div><span className="text-slate-300">Need best 4-bit quality?</span> <span className="text-amber-400">‚Üí GPTQ with calibration</span></div>
    </div>
    <div className="flex items-start gap-2">
      <span className="text-slate-500">‚Ä¢</span>
      <div><span className="text-slate-300">PTQ not good enough?</span> <span className="text-violet-400">‚Üí Try AWQ, then QAT</span></div>
    </div>
    <div className="flex items-start gap-2">
      <span className="text-slate-500">‚Ä¢</span>
      <div><span className="text-slate-300">RL policy?</span> <span className="text-emerald-400">‚Üí `int8` policy, `fp16` value</span></div>
    </div>
  </div>
</div>
