---
title: "Why Quantization Matters"
slug: "why-quantization"
section: "ML Concepts"
description: "Memory, speed, and the precision tradeoff"
status: "editor_reviewed"
lastReviewed: "2025-01-02"
---

import { Intuition, Mathematical, Implementation } from '@/components/ui/ContentLayers';
import { Note, Warning, Tip, Example, Definition } from '@/components/ui/Callouts';
import MemoryCalculator from '@/components/interactive/MemoryCalculator';

# Why Quantization Matters

Neural networks are getting bigger every year. LLaMA 3 has 405 billion parameters. Storing and running these models requires serious hardwareâ€”unless you quantize.

## The Memory Problem

**Try it yourself:** Select a model size and see how quantization shrinks memory requirements.

<MemoryCalculator client:load />

<Intuition>

**The math is simple:** Every parameter takes up space. `float32` uses 4 bytes per number.

A 7B model = 7 billion Ã— 4 bytes = **28 GB**.

But `int4` uses only 0.5 bytes per number. Same model = 7 billion Ã— 0.5 bytes = **3.5 GB**.

That's the difference between "needs a data center" and "runs on a laptop."

</Intuition>

<Mathematical>

Memory footprint:

$$\text{Memory} = N_{\text{params}} \times \frac{b}{8} \text{ bytes}$$

where $N_{\text{params}}$ is the parameter count and $b$ is bits per parameter.

</Mathematical>

## The Speed Problem

Memory size isn't just about fittingâ€”it's about **speed**.

<div className="my-8 p-6 bg-slate-800/30 rounded-xl border border-slate-700">
  <div className="text-slate-300 font-medium text-center mb-6">The GPU Bottleneck</div>

  <div className="flex items-center justify-center gap-6 py-4">
    <div className="text-center">
      <div className="w-16 h-16 rounded-lg bg-slate-700 flex items-center justify-center mb-2">
        <span className="text-2xl">ðŸ“¦</span>
      </div>
      <div className="text-sm text-slate-400">Memory</div>
      <div className="text-xs text-slate-500">28 GB weights</div>
    </div>

    <div className="flex flex-col items-center">
      <div className="text-amber-400 text-2xl mb-1">â†’â†’â†’</div>
      <div className="px-3 py-1 bg-red-900/50 border border-red-500/50 rounded text-xs text-red-300">
        Bandwidth Limited
      </div>
    </div>

    <div className="text-center">
      <div className="w-16 h-16 rounded-lg bg-emerald-900/40 border border-emerald-600/50 flex items-center justify-center mb-2">
        <span className="text-2xl">âš¡</span>
      </div>
      <div className="text-sm text-slate-400">Compute</div>
      <div className="text-xs text-emerald-400">Fast!</div>
    </div>
  </div>

  <div className="text-sm text-slate-400 text-center mt-4">
    For each token, the GPU loads <em>all</em> weights from memory.<br/>
    <strong className="text-slate-200">Smaller weights = faster loading = more tokens/sec.</strong>
  </div>
</div>

<Tip title="Why load weights every time?">

GPU compute cores have tiny caches (a few MB). A 7B model is 28GB in `float32`â€”the weights simply don't fit. They must be streamed from memory for every forward pass. This is called being **memory-bound**: the GPU spends most of its time waiting for data, not computing.

</Tip>

<Example title="Real Numbers: A100 Token Generation">

**70B model in `float32`** (280 GB):
- Load time: 280 GB Ã· 2000 GB/s = 140 ms
- Result: ~7 tokens/second

**Same model in `int4`** (35 GB):
- Load time: 35 GB Ã· 2000 GB/s = 17.5 ms
- Result: ~57 tokens/second

**8Ã— faster** from quantization alone!

</Example>

## Why It Works

Here's the surprising part: neural networks barely notice.

<div className="grid md:grid-cols-3 gap-4 my-8">
  <div className="bg-gradient-to-br from-cyan-900/30 to-cyan-800/10 rounded-xl p-5 border border-cyan-700/50">
    <div className="text-3xl mb-2">ðŸ”¢</div>
    <div className="text-cyan-400 font-bold mb-2">Overparameterized</div>
    <div className="text-slate-400 text-sm">Networks have way more parameters than needed. Small noise doesn't matter.</div>
  </div>
  <div className="bg-gradient-to-br from-violet-900/30 to-violet-800/10 rounded-xl p-5 border border-violet-700/50">
    <div className="text-3xl mb-2">ðŸ“‰</div>
    <div className="text-violet-400 font-bold mb-2">Flat Minima</div>
    <div className="text-slate-400 text-sm">Well-trained networks sit in regions where small weight changes don't affect outputs.</div>
  </div>
  <div className="bg-gradient-to-br from-amber-900/30 to-amber-800/10 rounded-xl p-5 border border-amber-700/50">
    <div className="text-3xl mb-2">ðŸŽ¯</div>
    <div className="text-amber-400 font-bold mb-2">Pattern Matters</div>
    <div className="text-slate-400 text-sm">The activation pattern is preserved even with reduced precision.</div>
  </div>
</div>

<Implementation>

```python
import torch
import torch.nn as nn

# Quick experiment: how much does int8 hurt?
torch.manual_seed(42)
layer = nn.Linear(512, 512)
x = torch.randn(32, 512)

with torch.no_grad():
    y_original = layer(x)

# Simple symmetric quantization
def quantize_int8(t):
    scale = t.abs().max() / 127
    return torch.round(t / scale).clamp(-128, 127) * scale

layer.weight.data = quantize_int8(layer.weight.data)

with torch.no_grad():
    y_quantized = layer(x)

error = (y_original - y_quantized).abs().mean() / y_original.abs().mean()
print(f"Relative error: {error:.2%}")  # Typically < 1%
```

</Implementation>

## When Does It Hurt?

<Warning title="Watch Out For">

<div className="grid md:grid-cols-2 gap-3 mt-4">
  <div className="flex items-start gap-2">
    <span className="text-red-400 text-lg">âœ—</span>
    <div>
      <div className="text-slate-200 font-medium">Outlier weights</div>
      <div className="text-slate-500 text-sm">A few large weights waste quantization range</div>
    </div>
  </div>
  <div className="flex items-start gap-2">
    <span className="text-red-400 text-lg">âœ—</span>
    <div>
      <div className="text-slate-200 font-medium">Math-heavy tasks</div>
      <div className="text-slate-500 text-sm">Precise calculations suffer more</div>
    </div>
  </div>
  <div className="flex items-start gap-2">
    <span className="text-red-400 text-lg">âœ—</span>
    <div>
      <div className="text-slate-200 font-medium">Very small models</div>
      <div className="text-slate-500 text-sm">Less redundancy to absorb error</div>
    </div>
  </div>
  <div className="flex items-start gap-2">
    <span className="text-red-400 text-lg">âœ—</span>
    <div>
      <div className="text-slate-200 font-medium">Extreme compression</div>
      <div className="text-slate-500 text-sm">int2/int3 needs special techniques</div>
    </div>
  </div>
</div>

<div className="mt-6 pt-4 border-t border-slate-700/50">

**Always measure accuracy after quantization.**

</div>

</Warning>

## Quick Reference

<div className="my-6">
  <div className="grid grid-cols-2 md:grid-cols-4 gap-3">
    <div className="bg-slate-800/50 rounded-lg p-4 text-center border border-slate-700">
      <div className="font-bold text-slate-300 font-mono">float32</div>
      <div className="text-xs text-slate-500 mt-1">Training baseline</div>
      <div className="text-xs text-slate-600 mt-2">Full precision</div>
    </div>
    <div className="bg-gradient-to-br from-blue-900/30 to-blue-800/10 rounded-lg p-4 text-center border border-blue-700/50">
      <div className="font-bold text-blue-400 font-mono">float16</div>
      <div className="text-xs text-slate-400 mt-1">Training + inference</div>
      <div className="text-xs text-blue-300 mt-2">2Ã— smaller</div>
    </div>
    <div className="bg-gradient-to-br from-emerald-900/30 to-emerald-800/10 rounded-lg p-4 text-center border border-emerald-700/50">
      <div className="font-bold text-emerald-400 font-mono">int8</div>
      <div className="text-xs text-slate-400 mt-1">Standard inference</div>
      <div className="text-xs text-emerald-300 mt-2">4Ã— smaller</div>
    </div>
    <div className="bg-gradient-to-br from-amber-900/30 to-amber-800/10 rounded-lg p-4 text-center border border-amber-700/50">
      <div className="font-bold text-amber-400 font-mono">int4</div>
      <div className="text-xs text-slate-400 mt-1">Edge / LLMs</div>
      <div className="text-xs text-amber-300 mt-2">8Ã— smaller</div>
    </div>
  </div>
</div>

<Tip title="Quantization in RL Pipelines">

RL systems have distinct phases with different precision needs:

- **Training neural networks**: Use `float16` or `bfloat16`. You need gradient precision, but full `float32` is overkill. Mixed-precision training gives you 2Ã— speedup with minimal accuracy loss.

- **Collecting rollouts**: Use `int8` for policy inference. When running thousands of parallel environments, inference speed matters more than perfect precision. The policy just needs to pick good actions.

- **Edge deployment**: Use `int4` when deploying to robots, drones, or embedded devices. Memory and power are constrained, and real-time response matters more than optimal actions.

</Tip>

## Next Up

<Note>

Now that you understand *why* quantization works, let's see *how* numbers are stored at the bit level.

Continue to [Number Representations](/chapters/quantization/number-representations).

</Note>
