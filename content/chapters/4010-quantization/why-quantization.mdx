---
title: "Why Quantization Matters"
slug: "why-quantization"
section: "ML Concepts"
description: "Memory, speed, and the precision tradeoff"
---

import { Intuition, Mathematical, Implementation } from '@/components/ui/ContentLayers';
import { Note, Warning, Tip, Example, Definition } from '@/components/ui/Callouts';
import MemoryCalculator from '@/components/interactive/MemoryCalculator';

# Why Quantization Matters

Neural networks are getting bigger every year. LLaMA 3 has 405 billion parameters. Storing and running these models requires serious hardwareâ€”unless you quantize.

## The Memory Problem

**Try it yourself:** Select a model size and see how quantization shrinks memory requirements.

<MemoryCalculator client:load />

<Intuition>

**The math is simple:** Every parameter takes up space. float32 uses 4 bytes per number.

A 7B model = 7 billion Ã— 4 bytes = **28 GB**.

But int4 uses only 0.5 bytes per number. Same model = 7 billion Ã— 0.5 bytes = **3.5 GB**.

That's the difference between "needs a data center" and "runs on a laptop."

</Intuition>

<Mathematical>

Memory footprint:

$$\text{Memory} = N_{\text{params}} \times \frac{b}{8} \text{ bytes}$$

where $N_{\text{params}}$ is the parameter count and $b$ is bits per parameter.

</Mathematical>

## The Speed Problem

Memory size isn't just about fittingâ€”it's about **speed**.

<div className="my-8 p-6 bg-slate-800/30 rounded-xl border border-slate-700">
  <div className="text-center mb-4">
    <div className="text-slate-400 text-sm mb-2">The GPU Bottleneck</div>
  </div>

  <div className="flex items-center justify-center gap-4 mb-6">
    <div className="text-center">
      <div className="w-16 h-16 rounded-lg bg-slate-700 flex items-center justify-center mb-2">
        <span className="text-2xl">ðŸ“¦</span>
      </div>
      <div className="text-xs text-slate-500">Memory</div>
    </div>
    <div className="flex-1 max-w-[200px]">
      <div className="h-4 bg-gradient-to-r from-red-500/50 via-amber-500/50 to-emerald-500/50 rounded-full relative">
        <div className="absolute inset-0 flex items-center justify-center text-xs font-bold text-white">
          Bandwidth
        </div>
      </div>
      <div className="text-center text-xs text-slate-500 mt-1">This is the bottleneck</div>
    </div>
    <div className="text-center">
      <div className="w-16 h-16 rounded-lg bg-slate-700 flex items-center justify-center mb-2">
        <span className="text-2xl">âš¡</span>
      </div>
      <div className="text-xs text-slate-500">Compute</div>
    </div>
  </div>

  <div className="text-sm text-slate-400 text-center">
    Modern GPUs compute faster than they can load data.<br/>
    <strong className="text-slate-200">Smaller weights = faster loading = faster inference.</strong>
  </div>
</div>

<Example title="Real Numbers: A100 Token Generation">

**70B model in float32** (280 GB):
- Load time: 280 GB Ã· 2000 GB/s = 140 ms
- Result: ~7 tokens/second

**Same model in int4** (35 GB):
- Load time: 35 GB Ã· 2000 GB/s = 17.5 ms
- Result: ~57 tokens/second

**8Ã— faster** from quantization alone!

</Example>

## Why It Works

Here's the surprising part: neural networks barely notice.

<div className="grid md:grid-cols-3 gap-4 my-8">
  <div className="bg-gradient-to-br from-cyan-900/30 to-cyan-800/10 rounded-xl p-5 border border-cyan-700/50">
    <div className="text-3xl mb-2">ðŸ”¢</div>
    <div className="text-cyan-400 font-bold mb-2">Overparameterized</div>
    <div className="text-slate-400 text-sm">Networks have way more parameters than needed. Small noise doesn't matter.</div>
  </div>
  <div className="bg-gradient-to-br from-violet-900/30 to-violet-800/10 rounded-xl p-5 border border-violet-700/50">
    <div className="text-3xl mb-2">ðŸ“‰</div>
    <div className="text-violet-400 font-bold mb-2">Flat Minima</div>
    <div className="text-slate-400 text-sm">Well-trained networks sit in regions where small weight changes don't affect outputs.</div>
  </div>
  <div className="bg-gradient-to-br from-amber-900/30 to-amber-800/10 rounded-xl p-5 border border-amber-700/50">
    <div className="text-3xl mb-2">ðŸŽ¯</div>
    <div className="text-amber-400 font-bold mb-2">Pattern Matters</div>
    <div className="text-slate-400 text-sm">The activation pattern is preserved even with reduced precision.</div>
  </div>
</div>

<Implementation>

```python
import torch
import torch.nn as nn

# Quick experiment: how much does int8 hurt?
torch.manual_seed(42)
layer = nn.Linear(512, 512)
x = torch.randn(32, 512)

with torch.no_grad():
    y_original = layer(x)

# Simple symmetric quantization
def quantize_int8(t):
    scale = t.abs().max() / 127
    return torch.round(t / scale).clamp(-128, 127) * scale

layer.weight.data = quantize_int8(layer.weight.data)

with torch.no_grad():
    y_quantized = layer(x)

error = (y_original - y_quantized).abs().mean() / y_original.abs().mean()
print(f"Relative error: {error:.2%}")  # Typically < 1%
```

</Implementation>

## When Does It Hurt?

<Warning title="Watch Out For">

<div className="grid md:grid-cols-2 gap-3 mt-4">
  <div className="flex items-start gap-2">
    <span className="text-red-400 text-lg">âœ—</span>
    <div>
      <div className="text-slate-200 font-medium">Outlier weights</div>
      <div className="text-slate-500 text-sm">A few large weights waste quantization range</div>
    </div>
  </div>
  <div className="flex items-start gap-2">
    <span className="text-red-400 text-lg">âœ—</span>
    <div>
      <div className="text-slate-200 font-medium">Math-heavy tasks</div>
      <div className="text-slate-500 text-sm">Precise calculations suffer more</div>
    </div>
  </div>
  <div className="flex items-start gap-2">
    <span className="text-red-400 text-lg">âœ—</span>
    <div>
      <div className="text-slate-200 font-medium">Very small models</div>
      <div className="text-slate-500 text-sm">Less redundancy to absorb error</div>
    </div>
  </div>
  <div className="flex items-start gap-2">
    <span className="text-red-400 text-lg">âœ—</span>
    <div>
      <div className="text-slate-200 font-medium">Extreme compression</div>
      <div className="text-slate-500 text-sm">int2/int3 needs special techniques</div>
    </div>
  </div>
</div>

**Always measure accuracy after quantization.**

</Warning>

## Quick Reference

<div className="my-6">
  <div className="grid grid-cols-2 md:grid-cols-4 gap-3">
    <div className="bg-slate-800/50 rounded-lg p-4 text-center border border-slate-700">
      <div className="font-bold text-slate-300">float32</div>
      <div className="text-xs text-slate-500 mt-1">Training baseline</div>
      <div className="text-xs text-slate-600 mt-2">Full precision</div>
    </div>
    <div className="bg-gradient-to-br from-blue-900/30 to-blue-800/10 rounded-lg p-4 text-center border border-blue-700/50">
      <div className="font-bold text-blue-400">float16</div>
      <div className="text-xs text-slate-400 mt-1">Training + inference</div>
      <div className="text-xs text-blue-400/70 mt-2">~0% loss</div>
    </div>
    <div className="bg-gradient-to-br from-emerald-900/30 to-emerald-800/10 rounded-lg p-4 text-center border border-emerald-700/50">
      <div className="font-bold text-emerald-400">int8</div>
      <div className="text-xs text-slate-400 mt-1">Standard inference</div>
      <div className="text-xs text-emerald-400/70 mt-2">~1% loss</div>
    </div>
    <div className="bg-gradient-to-br from-amber-900/30 to-amber-800/10 rounded-lg p-4 text-center border border-amber-700/50">
      <div className="font-bold text-amber-400">int4</div>
      <div className="text-xs text-slate-400 mt-1">Edge / LLMs</div>
      <div className="text-xs text-amber-400/70 mt-2">1-5% loss</div>
    </div>
  </div>
</div>

<Tip title="For RL">

- **Training**: float16/bfloat16 for gradients
- **Rollouts**: int8 for throughput
- **Deployment**: int4 for edge devices

</Tip>

## Next Up

<Note>

Now that you understand *why* quantization works, let's see *how* numbers are stored at the bit level.

Continue to [Number Representations](/chapters/quantization/number-representations).

</Note>
